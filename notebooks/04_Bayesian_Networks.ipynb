{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1. Imports](#Imports)\n",
    "- [2. Generate data with binary values](#Generate-data-with-binary-values)\n",
    "- [3. Learn structure and fit training data](#Learn-structure-and-fit-training-data)\n",
    "- [4. Predict labels for test part](#Predict-labels-for-test-part)\n",
    "- [5. Common structure](#Common-structure)\n",
    "\n",
    "[Back to Chemfin](../Chemfin.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Required packages: [pomegranate](http://pomegranate.readthedocs.io/en/latest/), [numpy](http://www.numpy.org/), [pandas](https://pandas.pydata.org/), [scikit-learn](http://scikit-learn.org/), [networkx](http://dschult-networkx.readthedocs.io/en/latest/) (version $\\geq$ 2.0)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pomegranate import BayesianNetwork\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import bayesian_networks as bn\n",
    "import bn_predict\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from io_work import stringSplitByNumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data with binary values\n",
    "\n",
    "Output is to be saved as csv file.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/large_bayesian_networks'\n",
    "model_filename_prefix = 'model_td'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "filename_dataset2 = 'test2.npz'\n",
    "filename_cv = 'cv_indices.npz'\n",
    "\n",
    "filename_save = 'bn_dataset.csv'\n",
    "filename_save2 = 'bn_test2.csv'\n",
    "\n",
    "left_fraction = 30\n",
    "geN = 20\n",
    "\n",
    "data = bn.loadMatrix(data_dirname+filename_dataset, one_node=1, ignore_negative=0)\n",
    "data, tau = bn.thresholdMatrix(data, left_fraction=left_fraction, one_node=1)\n",
    "data.to_csv(data_dirname+filename_save)\n",
    "colnames = data.columns.values\n",
    "\n",
    "data = bn.loadMatrix(data_dirname+filename_dataset2, one_node=1, ignore_negative=0)\n",
    "data, tau = bn.thresholdMatrix(data, left_fraction=left_fraction, one_node=1, cut_const_cols=0)\n",
    "data = data[colnames]\n",
    "data.to_csv(data_dirname+filename_save2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn structure and fit training data\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/large_bayesian_networks/'\n",
    "model_filename_prefix = 'model_bn_'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "filename_dataset = 'bn_dataset.csv'\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "data = pd.read_csv(data_dirname+filename_dataset, index_col=0)\n",
    "\n",
    "bn.produceModelsForValidationToJSON(data, train_indices, model_dirname, filename_base=model_filename_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels for test part\n",
    "\n",
    "It is a very slow operation, thus we parallelize code with multiprocessing package and observe only validational datasets.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_jobs = 2\n",
    "\n",
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/large_bayesian_networks/'\n",
    "model_filename_prefix = 'model_bn'\n",
    "results_dirname = '../results/large_bayesian_networks/'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "filename_dataset = 'bn_dataset.csv'\n",
    "filename_dataset2 = 'bn_test2.csv'\n",
    "\n",
    "df = np.load(data_dirname + filename_cv)\n",
    "test_indices = df['test_indices']\n",
    "\n",
    "data = pd.read_csv(data_dirname+filename_dataset, index_col=0)\n",
    "colnames = data.columns.values\n",
    "data2 = pd.read_csv(data_dirname+filename_dataset2, index_col=0)\n",
    "data2 = data2[colnames]\n",
    "\n",
    "all_model_filenames = os.listdir(model_dirname)\n",
    "all_model_filenames = filter(lambda x: x.startswith(model_filename_prefix), all_model_filenames)\n",
    "all_model_filenames = filter(lambda x: x.endswith('.json'), all_model_filenames)\n",
    "all_model_filenames = sorted(all_model_filenames, key=stringSplitByNumbers)\n",
    "\n",
    "print \"We found %d models in %s directory\" % (len(all_model_filenames), model_dirname)\n",
    "index = 0\n",
    "for model_filename in all_model_filenames:    \n",
    "    print model_filename\n",
    "    y = data.iloc[:, 0].values\n",
    "    X = data.iloc[:, 1:].values\n",
    "    X_test = X[test_indices[index], :].copy()\n",
    "    y_test = y[test_indices[index]].copy()\n",
    "    X_test = X_test.astype(int)\n",
    "    X_test = np.hstack([np.nan*np.empty([y_test.size, 1]), X_test])\n",
    "    bn_predict.launcher(N_jobs, model_filename, X_test, y_test, model_dirname, results_dirname)\n",
    "    bn_predict.gatherResults(index, results_dirname, filename_dataset.split('.')[0])\n",
    "    \n",
    "    X_test = data2.iloc[:, 1:].values\n",
    "    y_test = data2.iloc[:, 0].values\n",
    "    X_test = X_test.astype(int)\n",
    "    X_test = np.hstack([np.nan*np.empty([y_test.size, 1]), X_test])\n",
    "    bn_predict.launcher(N_jobs, model_filename, X_test, y_test, model_dirname, results_dirname)\n",
    "    bn_predict.gatherResults(index, results_dirname, postfix=filename_dataset2.split('.')[0])\n",
    "    index += 1\n",
    "    print \"============= index %d ready =========\" % (index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common structure\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 25 models in ../models/large_bayesian_networks/ directory\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/large_bayesian_networks/'\n",
    "model_dirname_save = '../models/large_bayesian_networks/intersection/'\n",
    "model_filename_prefix = 'model_bn'\n",
    "save_filename = 'common_tree.pkl'\n",
    "\n",
    "filename_dataset = 'bn_dataset.csv'\n",
    "\n",
    "all_model_filenames = os.listdir(model_dirname)\n",
    "all_model_filenames = filter(lambda x: x.startswith(model_filename_prefix), all_model_filenames)\n",
    "all_model_filenames = filter(lambda x: x.endswith('.json'), all_model_filenames)\n",
    "all_model_filenames = sorted(all_model_filenames, key=stringSplitByNumbers)\n",
    "\n",
    "G = None\n",
    "root = 0\n",
    "\n",
    "print \"We found %d models in %s directory\" % (len(all_model_filenames), model_dirname)\n",
    "index = 0\n",
    "for model_filename in all_model_filenames:\n",
    "    model = BayesianNetwork.from_json(model_dirname+model_filename)\n",
    "    tree = nx.Graph()\n",
    "\n",
    "    dictTree = {}\n",
    "    for k in xrange(len(model.structure)):\n",
    "        dictTree[k] = model.structure[k]\n",
    "    nx.from_dict_of_lists(dictTree, create_using=tree)\n",
    "    if G is None:\n",
    "        G = copy.deepcopy(tree)\n",
    "    else:\n",
    "        G = nx.intersection(G, tree)\n",
    "G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "tree = nx.DiGraph()\n",
    "nx.from_edgelist(G.edges, create_using=tree)\n",
    "#tree = tree.reverse()\n",
    "#json_data = nx.readwrite.json_graph.tree_data(tree, root)\n",
    "nx.write_gpickle(tree, model_dirname_save+save_filename)\n",
    "# G = nx.read_gpickle(model_dirname_save+save_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
