{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [1. Imports](#Imports)\n",
    "- [2. Build models](#Build-models)\n",
    "- [3. Approximation error](#Approximation-error)\n",
    "- [4. Encode data with computed autoencoders](#Encode-data-with-computed-autoencoders)\n",
    "- [5. Investigate how size of last encoding layer affects results: training models](#Investigate-how-size-of-last-encoding-layer-affects-results:-training-models)\n",
    "- [6. Investigate how size of last encoding layer affects results: encode data](#Investigate-how-size-of-last-encoding-layer-affects-results:-encode-data)\n",
    "- [7. Investigate how size of last encoding layer affects results: predict with logistic regression](#Investigate-how-size-of-last-encoding-layer-affects-results:-predict-with-logistic-regression)\n",
    "- [8. Logistic regression classifier with encoded data](#Logistic-regression-classifier-with-encoded-data)\n",
    "- [9. Gaussian Naive Bayes classifier with encoded data](#Gaussian-Naive-Bayes-classifier-with-encoded-data)\n",
    "- [10. Hybrid Bayesian classifier with bnlearn](#Hybrid-Bayesian-classifier-with-bnlearn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Back to Chemfin](../Chemfin.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The first cell with code includes all necessary inputs.\n",
    "\n",
    "Requires [numpy](http://www.numpy.org/), [scikit-learn](http://scikit-learn.org/), [pyTorch](http://pytorch.org/), [Rpy2](https://rpy2.readthedocs.io).\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "mkl_rt = ctypes.CDLL('libmkl_rt.so')\n",
    "print(mkl_rt.mkl_get_max_threads())\n",
    "mkl_get_max_threads = mkl_rt.mkl_get_max_threads\n",
    "def mkl_set_num_threads(cores):\n",
    "    mkl_rt.mkl_set_num_threads(ctypes.byref(ctypes.c_int(cores)))\n",
    "\n",
    "mkl_set_num_threads(4)\n",
    "print(mkl_get_max_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=4\n",
      "env: OMP_NUM_THREADS=4\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=4\n",
    "%env OMP_NUM_THREADS=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%env OMP_NUM_THREADS=8\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "torch.set_num_threads(8)\n",
    "from torch import nn\n",
    "torch.set_num_threads(4)\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import autoencoder as ae\n",
    "\n",
    "random_state = 150\n",
    "torch.manual_seed(random_state);\n",
    "\n",
    "\n",
    "from computational_utils import reshape\n",
    "import bayesian_networks as bn\n",
    "\n",
    "from io_work import stringSplitByNumbers\n",
    "\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from computational_utils import reshape\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models\n",
    "\n",
    "Next cell contains script to build autoencoder models relevant to CV indexes supplied by data/cv_indices.npz.\n",
    "\n",
    "Parameters to control are:\n",
    "\n",
    "- sizes: list of integers which specifies output sizes for each encoding layer\n",
    "- batch_size: number of samples to be used for computing new update at each epoch\n",
    "- nEpoch: number of epochs for each layer\n",
    "- num_workers: number of parallel processes to work\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full\n",
      "============= CV 1 / 25 ================\n",
      "(1) Errors on training set (1708 samples): \n",
      "min=8.428e-02 / mean=1.438e-01 / median=1.384e-01 / max=3.616e-01\n",
      "(1) Errors on validation set (154 samples): \n",
      "min=9.462e-02 / mean=2.482e-01 / median=2.193e-01 / max=7.921e-01\n",
      "(2) Errors on training set (1708 samples): \n",
      "min=5.450e-02 / mean=9.692e-02 / median=9.392e-02 / max=2.032e-01\n",
      "(2) Errors on validation set (154 samples): \n",
      "min=5.876e-02 / mean=2.430e-01 / median=2.078e-01 / max=7.188e-01\n",
      "(3) Errors on training set (1708 samples): \n",
      "min=5.335e-02 / mean=1.079e-01 / median=1.035e-01 / max=2.370e-01\n",
      "(3) Errors on validation set (154 samples): \n",
      "min=5.860e-02 / mean=2.960e-01 / median=2.574e-01 / max=8.762e-01\n",
      "============= CV 2 / 25 ================\n",
      "(1) Errors on training set (1751 samples): \n",
      "min=8.201e-02 / mean=1.444e-01 / median=1.389e-01 / max=5.689e-01\n",
      "(1) Errors on validation set (145 samples): \n",
      "min=1.042e-01 / mean=2.228e-01 / median=1.963e-01 / max=7.212e-01\n",
      "(2) Errors on training set (1751 samples): \n",
      "min=4.348e-02 / mean=9.191e-02 / median=8.939e-02 / max=2.129e-01\n",
      "(2) Errors on validation set (145 samples): \n",
      "min=6.793e-02 / mean=2.068e-01 / median=1.673e-01 / max=6.328e-01\n",
      "(3) Errors on training set (1751 samples): \n",
      "min=4.946e-02 / mean=1.085e-01 / median=1.057e-01 / max=2.953e-01\n",
      "(3) Errors on validation set (145 samples): \n",
      "min=6.976e-02 / mean=2.467e-01 / median=1.992e-01 / max=9.702e-01\n",
      "============= CV 3 / 25 ================\n",
      "(1) Errors on training set (1797 samples): \n",
      "min=8.388e-02 / mean=1.419e-01 / median=1.367e-01 / max=4.542e-01\n",
      "(1) Errors on validation set (133 samples): \n",
      "min=8.607e-02 / mean=2.280e-01 / median=2.037e-01 / max=8.215e-01\n",
      "(2) Errors on training set (1797 samples): \n",
      "min=4.755e-02 / mean=8.948e-02 / median=8.633e-02 / max=2.021e-01\n",
      "(2) Errors on validation set (133 samples): \n",
      "min=4.860e-02 / mean=2.083e-01 / median=1.634e-01 / max=6.273e-01\n",
      "(3) Errors on training set (1797 samples): \n",
      "min=5.090e-02 / mean=1.095e-01 / median=1.042e-01 / max=2.588e-01\n",
      "(3) Errors on validation set (133 samples): \n",
      "min=5.927e-02 / mean=2.508e-01 / median=2.077e-01 / max=6.981e-01\n",
      "============= CV 4 / 25 ================\n",
      "(1) Errors on training set (1875 samples): \n",
      "min=8.178e-02 / mean=1.311e-01 / median=1.266e-01 / max=2.774e-01\n",
      "(1) Errors on validation set (113 samples): \n",
      "min=1.081e-01 / mean=2.238e-01 / median=1.999e-01 / max=4.875e-01\n",
      "(2) Errors on training set (1875 samples): \n",
      "min=4.414e-02 / mean=9.064e-02 / median=8.792e-02 / max=2.009e-01\n",
      "(2) Errors on validation set (113 samples): \n",
      "min=6.817e-02 / mean=2.107e-01 / median=1.806e-01 / max=5.588e-01\n",
      "(3) Errors on training set (1875 samples): \n",
      "min=4.767e-02 / mean=1.058e-01 / median=1.021e-01 / max=3.599e-01\n",
      "(3) Errors on validation set (113 samples): \n",
      "min=8.045e-02 / mean=2.671e-01 / median=2.188e-01 / max=7.499e-01\n",
      "============= CV 5 / 25 ================\n",
      "(1) Errors on training set (1917 samples): \n",
      "min=7.121e-02 / mean=1.285e-01 / median=1.242e-01 / max=2.674e-01\n",
      "(1) Errors on validation set (107 samples): \n",
      "min=8.514e-02 / mean=2.294e-01 / median=2.007e-01 / max=7.248e-01\n",
      "(2) Errors on training set (1917 samples): \n",
      "min=4.224e-02 / mean=8.851e-02 / median=8.519e-02 / max=1.918e-01\n",
      "(2) Errors on validation set (107 samples): \n",
      "min=5.863e-02 / mean=2.258e-01 / median=1.917e-01 / max=6.637e-01\n",
      "(3) Errors on training set (1917 samples): \n",
      "min=4.777e-02 / mean=1.074e-01 / median=1.027e-01 / max=2.567e-01\n",
      "(3) Errors on validation set (107 samples): \n",
      "min=5.558e-02 / mean=2.779e-01 / median=2.314e-01 / max=7.432e-01\n",
      "============= CV 6 / 25 ================\n",
      "(1) Errors on training set (1709 samples): \n",
      "min=8.127e-02 / mean=1.451e-01 / median=1.389e-01 / max=5.909e-01\n",
      "(1) Errors on validation set (154 samples): \n",
      "min=9.455e-02 / mean=2.349e-01 / median=2.059e-01 / max=6.029e-01\n",
      "(2) Errors on training set (1709 samples): \n",
      "min=4.840e-02 / mean=9.270e-02 / median=8.924e-02 / max=1.922e-01\n",
      "(2) Errors on validation set (154 samples): \n",
      "min=5.451e-02 / mean=2.187e-01 / median=1.680e-01 / max=7.015e-01\n",
      "(3) Errors on training set (1709 samples): \n",
      "min=5.573e-02 / mean=1.097e-01 / median=1.062e-01 / max=3.270e-01\n",
      "(3) Errors on validation set (154 samples): \n",
      "min=5.468e-02 / mean=2.639e-01 / median=2.168e-01 / max=8.098e-01\n",
      "============= CV 7 / 25 ================\n",
      "(1) Errors on training set (1749 samples): \n",
      "min=7.643e-02 / mean=1.435e-01 / median=1.381e-01 / max=4.031e-01\n",
      "(1) Errors on validation set (145 samples): \n",
      "min=8.052e-02 / mean=2.439e-01 / median=2.127e-01 / max=7.651e-01\n",
      "(2) Errors on training set (1749 samples): \n",
      "min=4.377e-02 / mean=8.883e-02 / median=8.623e-02 / max=1.942e-01\n",
      "(2) Errors on validation set (145 samples): \n",
      "min=4.340e-02 / mean=2.300e-01 / median=2.040e-01 / max=8.037e-01\n",
      "(3) Errors on training set (1749 samples): \n",
      "min=5.074e-02 / mean=1.094e-01 / median=1.049e-01 / max=5.028e-01\n",
      "(3) Errors on validation set (145 samples): \n",
      "min=6.038e-02 / mean=2.789e-01 / median=2.522e-01 / max=8.441e-01\n",
      "============= CV 8 / 25 ================\n",
      "(1) Errors on training set (1834 samples): \n",
      "min=8.195e-02 / mean=1.357e-01 / median=1.307e-01 / max=4.470e-01\n",
      "(1) Errors on validation set (133 samples): \n",
      "min=9.545e-02 / mean=2.053e-01 / median=1.825e-01 / max=5.534e-01\n",
      "(2) Errors on training set (1834 samples): \n",
      "min=5.557e-02 / mean=9.684e-02 / median=9.350e-02 / max=1.978e-01\n",
      "(2) Errors on validation set (133 samples): \n",
      "min=6.160e-02 / mean=1.951e-01 / median=1.543e-01 / max=7.187e-01\n",
      "(3) Errors on training set (1834 samples): \n",
      "min=6.307e-02 / mean=1.232e-01 / median=1.190e-01 / max=3.168e-01\n",
      "(3) Errors on validation set (133 samples): \n",
      "min=7.093e-02 / mean=2.407e-01 / median=2.015e-01 / max=8.124e-01\n",
      "============= CV 9 / 25 ================\n",
      "(1) Errors on training set (1873 samples): \n",
      "min=7.751e-02 / mean=1.312e-01 / median=1.262e-01 / max=4.016e-01\n",
      "(1) Errors on validation set (113 samples): \n",
      "min=1.066e-01 / mean=2.301e-01 / median=1.945e-01 / max=8.088e-01\n",
      "(2) Errors on training set (1873 samples): \n",
      "min=4.820e-02 / mean=8.977e-02 / median=8.706e-02 / max=2.509e-01\n",
      "(2) Errors on validation set (113 samples): \n",
      "min=6.996e-02 / mean=2.266e-01 / median=1.872e-01 / max=7.204e-01\n",
      "(3) Errors on training set (1873 samples): \n",
      "min=5.962e-02 / mean=1.121e-01 / median=1.087e-01 / max=3.140e-01\n",
      "(3) Errors on validation set (113 samples): \n",
      "min=8.500e-02 / mean=2.704e-01 / median=2.392e-01 / max=7.571e-01\n",
      "============= CV 10 / 25 ================\n",
      "(1) Errors on training set (1883 samples): \n",
      "min=7.534e-02 / mean=1.292e-01 / median=1.252e-01 / max=2.812e-01\n",
      "(1) Errors on validation set (107 samples): \n",
      "min=8.637e-02 / mean=2.303e-01 / median=1.940e-01 / max=8.202e-01\n",
      "(2) Errors on training set (1883 samples): \n",
      "min=4.969e-02 / mean=9.299e-02 / median=9.046e-02 / max=1.874e-01\n",
      "(2) Errors on validation set (107 samples): \n",
      "min=6.764e-02 / mean=2.275e-01 / median=1.947e-01 / max=7.221e-01\n",
      "(3) Errors on training set (1883 samples): \n",
      "min=6.116e-02 / mean=1.111e-01 / median=1.063e-01 / max=2.872e-01\n",
      "(3) Errors on validation set (107 samples): \n",
      "min=8.230e-02 / mean=2.860e-01 / median=2.320e-01 / max=8.432e-01\n",
      "============= CV 11 / 25 ================\n",
      "(1) Errors on training set (1725 samples): \n",
      "min=8.318e-02 / mean=1.437e-01 / median=1.381e-01 / max=4.466e-01\n",
      "(1) Errors on validation set (154 samples): \n",
      "min=8.533e-02 / mean=2.353e-01 / median=2.041e-01 / max=6.123e-01\n",
      "(2) Errors on training set (1725 samples): \n",
      "min=4.464e-02 / mean=9.144e-02 / median=8.827e-02 / max=2.190e-01\n",
      "(2) Errors on validation set (154 samples): \n",
      "min=4.839e-02 / mean=2.237e-01 / median=1.821e-01 / max=6.713e-01\n",
      "(3) Errors on training set (1725 samples): \n",
      "min=4.809e-02 / mean=1.095e-01 / median=1.046e-01 / max=3.791e-01\n",
      "(3) Errors on validation set (154 samples): \n",
      "min=5.961e-02 / mean=2.734e-01 / median=2.237e-01 / max=9.219e-01\n",
      "============= CV 12 / 25 ================\n",
      "(1) Errors on training set (1755 samples): \n",
      "min=8.250e-02 / mean=1.423e-01 / median=1.380e-01 / max=3.816e-01\n",
      "(1) Errors on validation set (145 samples): \n",
      "min=9.735e-02 / mean=2.298e-01 / median=1.934e-01 / max=7.854e-01\n",
      "(2) Errors on training set (1755 samples): \n",
      "min=4.769e-02 / mean=9.181e-02 / median=8.863e-02 / max=2.104e-01\n",
      "(2) Errors on validation set (145 samples): \n",
      "min=6.213e-02 / mean=2.238e-01 / median=1.707e-01 / max=7.566e-01\n",
      "(3) Errors on training set (1755 samples): \n",
      "min=5.050e-02 / mean=1.095e-01 / median=1.054e-01 / max=3.490e-01\n",
      "(3) Errors on validation set (145 samples): \n",
      "min=7.326e-02 / mean=2.608e-01 / median=2.008e-01 / max=8.131e-01\n",
      "============= CV 13 / 25 ================\n",
      "(1) Errors on training set (1792 samples): \n",
      "min=7.779e-02 / mean=1.420e-01 / median=1.371e-01 / max=3.417e-01\n",
      "(1) Errors on validation set (133 samples): \n",
      "min=8.224e-02 / mean=2.306e-01 / median=1.916e-01 / max=6.774e-01\n",
      "(2) Errors on training set (1792 samples): \n",
      "min=4.472e-02 / mean=9.113e-02 / median=8.785e-02 / max=1.935e-01\n",
      "(2) Errors on validation set (133 samples): \n",
      "min=5.071e-02 / mean=2.170e-01 / median=1.744e-01 / max=6.398e-01\n"
     ]
    }
   ],
   "source": [
    "#truncated_features = True\n",
    "\n",
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "filename_dataset = 'dataset.npz'\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "\n",
    "'''\n",
    "if truncated_features:\n",
    "    model_filename_prefix = 'model_ae_truncated_'\n",
    "\n",
    "    left_fraction = 30\n",
    "    geN = 20\n",
    "\n",
    "    data = bn.loadMatrix(data_dirname+filename_dataset, one_node=1, ignore_negative=0)\n",
    "    data, tau = bn.thresholdMatrix(data, left_fraction=left_fraction, one_node=1)\n",
    "    T, labels = data.iloc[:, 1:].values, data.iloc[:, 0].values\n",
    "    print 'truncated'\n",
    "else:\n",
    "'''\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "print 'full'\n",
    "    \n",
    "sizes = [400, 100, 25]\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 1\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "\n",
    "\n",
    "ae.buildAutoencoderModels(\n",
    "    T, train_indices, test_indices, sizes, model_dirname, nEpoch,\n",
    "    batch_size, num_workers, model_filename_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation error\n",
    "\n",
    "In this code data encoded and decoded with previously trained models. Resulting approximation (relative residual error by means of $l_2$ norm) is printed sample-wise.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "model_ae_0\n",
      "Training set:\n",
      "min=5.335e-02 / mean=1.079e-01 / median=1.035e-01 / max=2.370e-01\n",
      "Validation set:\n",
      "min=5.860e-02 / mean=2.960e-01 / median=2.574e-01 / max=8.762e-01\n",
      "========================\n",
      "model_ae_1\n",
      "Training set:\n",
      "min=4.946e-02 / mean=1.085e-01 / median=1.057e-01 / max=2.953e-01\n",
      "Validation set:\n",
      "min=6.976e-02 / mean=2.467e-01 / median=1.992e-01 / max=9.702e-01\n",
      "========================\n",
      "model_ae_2\n",
      "Training set:\n",
      "min=5.090e-02 / mean=1.095e-01 / median=1.042e-01 / max=2.588e-01\n",
      "Validation set:\n",
      "min=5.927e-02 / mean=2.508e-01 / median=2.077e-01 / max=6.981e-01\n",
      "========================\n",
      "model_ae_3\n",
      "Training set:\n",
      "min=4.767e-02 / mean=1.058e-01 / median=1.021e-01 / max=3.599e-01\n",
      "Validation set:\n",
      "min=8.045e-02 / mean=2.671e-01 / median=2.188e-01 / max=7.499e-01\n",
      "========================\n",
      "model_ae_4\n",
      "Training set:\n",
      "min=4.777e-02 / mean=1.074e-01 / median=1.027e-01 / max=2.567e-01\n",
      "Validation set:\n",
      "min=5.558e-02 / mean=2.779e-01 / median=2.314e-01 / max=7.432e-01\n",
      "========================\n",
      "model_ae_5\n",
      "Training set:\n",
      "min=5.573e-02 / mean=1.097e-01 / median=1.062e-01 / max=3.270e-01\n",
      "Validation set:\n",
      "min=5.468e-02 / mean=2.639e-01 / median=2.168e-01 / max=8.098e-01\n",
      "========================\n",
      "model_ae_6\n",
      "Training set:\n",
      "min=5.074e-02 / mean=1.094e-01 / median=1.049e-01 / max=5.028e-01\n",
      "Validation set:\n",
      "min=6.038e-02 / mean=2.789e-01 / median=2.522e-01 / max=8.441e-01\n",
      "========================\n",
      "model_ae_7\n",
      "Training set:\n",
      "min=6.307e-02 / mean=1.232e-01 / median=1.190e-01 / max=3.168e-01\n",
      "Validation set:\n",
      "min=7.093e-02 / mean=2.407e-01 / median=2.015e-01 / max=8.124e-01\n",
      "========================\n",
      "model_ae_8\n",
      "Training set:\n",
      "min=5.962e-02 / mean=1.121e-01 / median=1.087e-01 / max=3.140e-01\n",
      "Validation set:\n",
      "min=8.500e-02 / mean=2.704e-01 / median=2.392e-01 / max=7.571e-01\n",
      "========================\n",
      "model_ae_9\n",
      "Training set:\n",
      "min=6.116e-02 / mean=1.111e-01 / median=1.063e-01 / max=2.872e-01\n",
      "Validation set:\n",
      "min=8.230e-02 / mean=2.860e-01 / median=2.320e-01 / max=8.432e-01\n",
      "========================\n",
      "model_ae_10\n",
      "Training set:\n",
      "min=4.809e-02 / mean=1.095e-01 / median=1.046e-01 / max=3.791e-01\n",
      "Validation set:\n",
      "min=5.961e-02 / mean=2.734e-01 / median=2.237e-01 / max=9.219e-01\n",
      "========================\n",
      "model_ae_11\n",
      "Training set:\n",
      "min=5.050e-02 / mean=1.095e-01 / median=1.054e-01 / max=3.490e-01\n",
      "Validation set:\n",
      "min=7.326e-02 / mean=2.608e-01 / median=2.008e-01 / max=8.131e-01\n",
      "========================\n",
      "model_ae_12\n",
      "Training set:\n",
      "min=4.927e-02 / mean=1.076e-01 / median=1.037e-01 / max=2.752e-01\n",
      "Validation set:\n",
      "min=5.580e-02 / mean=2.728e-01 / median=2.054e-01 / max=7.897e-01\n",
      "========================\n",
      "model_ae_13\n",
      "Training set:\n",
      "min=5.249e-02 / mean=1.096e-01 / median=1.054e-01 / max=2.769e-01\n",
      "Validation set:\n",
      "min=5.794e-02 / mean=2.591e-01 / median=2.093e-01 / max=8.379e-01\n",
      "========================\n",
      "model_ae_14\n",
      "Training set:\n",
      "min=5.804e-02 / mean=1.096e-01 / median=1.059e-01 / max=2.282e-01\n",
      "Validation set:\n",
      "min=6.797e-02 / mean=2.605e-01 / median=2.146e-01 / max=1.029e+00\n",
      "========================\n",
      "model_ae_15\n",
      "Training set:\n",
      "min=4.813e-02 / mean=1.106e-01 / median=1.066e-01 / max=2.553e-01\n",
      "Validation set:\n",
      "min=7.523e-02 / mean=2.760e-01 / median=2.309e-01 / max=8.344e-01\n",
      "========================\n",
      "model_ae_16\n",
      "Training set:\n",
      "min=5.312e-02 / mean=1.118e-01 / median=1.082e-01 / max=3.231e-01\n",
      "Validation set:\n",
      "min=7.158e-02 / mean=2.561e-01 / median=1.967e-01 / max=9.471e-01\n",
      "========================\n",
      "model_ae_17\n",
      "Training set:\n",
      "min=5.775e-02 / mean=1.156e-01 / median=1.125e-01 / max=2.899e-01\n",
      "Validation set:\n",
      "min=7.083e-02 / mean=2.653e-01 / median=2.001e-01 / max=7.991e-01\n",
      "========================\n",
      "model_ae_18\n",
      "Training set:\n",
      "min=5.875e-02 / mean=1.084e-01 / median=1.044e-01 / max=4.169e-01\n",
      "Validation set:\n",
      "min=6.652e-02 / mean=2.603e-01 / median=2.079e-01 / max=8.154e-01\n",
      "========================\n",
      "model_ae_19\n",
      "Training set:\n",
      "min=6.314e-02 / mean=1.207e-01 / median=1.164e-01 / max=3.250e-01\n",
      "Validation set:\n",
      "min=8.335e-02 / mean=2.805e-01 / median=2.268e-01 / max=7.905e-01\n",
      "========================\n",
      "model_ae_20\n",
      "Training set:\n",
      "min=5.289e-02 / mean=1.107e-01 / median=1.068e-01 / max=3.240e-01\n",
      "Validation set:\n",
      "min=5.654e-02 / mean=2.612e-01 / median=2.193e-01 / max=7.713e-01\n",
      "========================\n",
      "model_ae_21\n",
      "Training set:\n",
      "min=5.431e-02 / mean=1.087e-01 / median=1.045e-01 / max=3.376e-01\n",
      "Validation set:\n",
      "min=6.514e-02 / mean=2.736e-01 / median=2.188e-01 / max=7.979e-01\n",
      "========================\n",
      "model_ae_22\n",
      "Training set:\n",
      "min=4.189e-02 / mean=1.096e-01 / median=1.047e-01 / max=3.086e-01\n",
      "Validation set:\n",
      "min=5.505e-02 / mean=2.722e-01 / median=2.228e-01 / max=8.083e-01\n",
      "========================\n",
      "model_ae_23\n",
      "Training set:\n",
      "min=5.680e-02 / mean=1.128e-01 / median=1.091e-01 / max=3.087e-01\n",
      "Validation set:\n",
      "min=6.087e-02 / mean=2.794e-01 / median=2.298e-01 / max=8.478e-01\n",
      "========================\n",
      "model_ae_24\n",
      "Training set:\n",
      "min=5.883e-02 / mean=1.121e-01 / median=1.091e-01 / max=3.295e-01\n",
      "Validation set:\n",
      "min=7.055e-02 / mean=2.588e-01 / median=2.084e-01 / max=9.112e-01\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "N = T.shape[1]\n",
    "\n",
    "ae.checkRelRes(T, train_indices, test_indices, sizes, model_dirname, model_filename_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data with computed autoencoders\n",
    "\n",
    "It will produce data encoded with models from previous steps.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded with model_ae_0. Comp.time=0.23902\n",
      "encoded with model_ae_1. Comp.time=0.23884\n",
      "encoded with model_ae_2. Comp.time=0.24471\n",
      "encoded with model_ae_3. Comp.time=0.24480\n",
      "encoded with model_ae_4. Comp.time=0.24264\n",
      "encoded with model_ae_5. Comp.time=0.23970\n",
      "encoded with model_ae_6. Comp.time=0.24314\n",
      "encoded with model_ae_7. Comp.time=0.24486\n",
      "encoded with model_ae_8. Comp.time=0.23966\n",
      "encoded with model_ae_9. Comp.time=0.24009\n",
      "encoded with model_ae_10. Comp.time=0.23975\n",
      "encoded with model_ae_11. Comp.time=0.24006\n",
      "encoded with model_ae_12. Comp.time=0.23938\n",
      "encoded with model_ae_13. Comp.time=0.24465\n",
      "encoded with model_ae_14. Comp.time=0.24483\n",
      "encoded with model_ae_15. Comp.time=0.24414\n",
      "encoded with model_ae_16. Comp.time=0.23968\n",
      "encoded with model_ae_17. Comp.time=0.23913\n",
      "encoded with model_ae_18. Comp.time=0.23983\n",
      "encoded with model_ae_19. Comp.time=0.23939\n",
      "encoded with model_ae_20. Comp.time=0.23964\n",
      "encoded with model_ae_21. Comp.time=0.23965\n",
      "encoded with model_ae_22. Comp.time=0.23946\n",
      "encoded with model_ae_23. Comp.time=0.23909\n",
      "encoded with model_ae_24. Comp.time=0.23940\n",
      "encoded with model_ae_0. Comp.time=0.00945\n",
      "encoded with model_ae_1. Comp.time=0.00973\n",
      "encoded with model_ae_2. Comp.time=0.01001\n",
      "encoded with model_ae_3. Comp.time=0.00985\n",
      "encoded with model_ae_4. Comp.time=0.01023\n",
      "encoded with model_ae_5. Comp.time=0.01017\n",
      "encoded with model_ae_6. Comp.time=0.01018\n",
      "encoded with model_ae_7. Comp.time=0.01024\n",
      "encoded with model_ae_8. Comp.time=0.00988\n",
      "encoded with model_ae_9. Comp.time=0.00979\n",
      "encoded with model_ae_10. Comp.time=0.01002\n",
      "encoded with model_ae_11. Comp.time=0.01020\n",
      "encoded with model_ae_12. Comp.time=0.01018\n",
      "encoded with model_ae_13. Comp.time=0.01009\n",
      "encoded with model_ae_14. Comp.time=0.01592\n",
      "encoded with model_ae_15. Comp.time=0.00993\n",
      "encoded with model_ae_16. Comp.time=0.00993\n",
      "encoded with model_ae_17. Comp.time=0.01014\n",
      "encoded with model_ae_18. Comp.time=0.01010\n",
      "encoded with model_ae_19. Comp.time=0.01011\n",
      "encoded with model_ae_20. Comp.time=0.01020\n",
      "encoded with model_ae_21. Comp.time=0.01462\n",
      "encoded with model_ae_22. Comp.time=0.01003\n",
      "encoded with model_ae_23. Comp.time=0.00988\n",
      "encoded with model_ae_24. Comp.time=0.00996\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "#model_filename_prefix = 'model_aeGE20_'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset = 'dataset.npz'\n",
    "save_filename = 'autoencoded_' + filename_dataset\n",
    "filename_dataset2 = 'test2.npz'\n",
    "save_filename2 = 'autoencoded_' + filename_dataset2\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "num_workers = 1\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "ae.encodeDataset(df, sizes, model_dirname, model_filename_prefix,\n",
    "                  data_dirname+save_filename, num_workers, return_result=0)\n",
    "df = np.load(data_dirname+filename_dataset2)\n",
    "ae.encodeDataset(df, sizes, model_dirname, model_filename_prefix,\n",
    "                  data_dirname+save_filename2, num_workers, return_result=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: training models\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= CV 1 / 5 ================\n",
      "(1) Errors on training set (1708 samples): \n",
      "min=8.584e-02 / mean=1.455e-01 / median=1.397e-01 / max=4.386e-01\n",
      "(1) Errors on validation set (154 samples): \n",
      "min=9.600e-02 / mean=2.503e-01 / median=2.243e-01 / max=8.443e-01\n",
      "(2) Errors on training set (1708 samples): \n",
      "min=4.957e-02 / mean=9.409e-02 / median=9.081e-02 / max=2.073e-01\n",
      "(2) Errors on validation set (154 samples): \n",
      "min=5.442e-02 / mean=2.427e-01 / median=2.100e-01 / max=7.229e-01\n",
      "============= CV 2 / 5 ================\n",
      "(1) Errors on training set (1751 samples): \n",
      "min=8.259e-02 / mean=1.439e-01 / median=1.392e-01 / max=4.110e-01\n",
      "(1) Errors on validation set (145 samples): \n",
      "min=1.063e-01 / mean=2.231e-01 / median=1.967e-01 / max=7.348e-01\n",
      "(2) Errors on training set (1751 samples): \n",
      "min=4.329e-02 / mean=9.175e-02 / median=8.890e-02 / max=2.614e-01\n",
      "(2) Errors on validation set (145 samples): \n",
      "min=6.902e-02 / mean=2.036e-01 / median=1.663e-01 / max=5.968e-01\n",
      "============= CV 3 / 5 ================\n",
      "(1) Errors on training set (1797 samples): \n",
      "min=8.300e-02 / mean=1.413e-01 / median=1.361e-01 / max=3.367e-01\n",
      "(1) Errors on validation set (133 samples): \n",
      "min=8.603e-02 / mean=2.255e-01 / median=1.980e-01 / max=7.275e-01\n",
      "(2) Errors on training set (1797 samples): \n",
      "min=4.810e-02 / mean=9.154e-02 / median=8.900e-02 / max=2.025e-01\n",
      "(2) Errors on validation set (133 samples): \n",
      "min=4.569e-02 / mean=2.079e-01 / median=1.668e-01 / max=5.993e-01\n",
      "============= CV 4 / 5 ================\n",
      "(1) Errors on training set (1875 samples): \n",
      "min=7.802e-02 / mean=1.302e-01 / median=1.249e-01 / max=2.832e-01\n",
      "(1) Errors on validation set (113 samples): \n",
      "min=1.046e-01 / mean=2.245e-01 / median=2.041e-01 / max=5.028e-01\n",
      "(2) Errors on training set (1875 samples): \n",
      "min=4.283e-02 / mean=9.140e-02 / median=8.907e-02 / max=1.799e-01\n",
      "(2) Errors on validation set (113 samples): \n",
      "min=7.387e-02 / mean=2.124e-01 / median=1.815e-01 / max=5.373e-01\n",
      "============= CV 5 / 5 ================\n",
      "(1) Errors on training set (1917 samples): \n",
      "min=7.363e-02 / mean=1.287e-01 / median=1.242e-01 / max=3.501e-01\n",
      "(1) Errors on validation set (107 samples): \n",
      "min=8.405e-02 / mean=2.296e-01 / median=2.043e-01 / max=6.869e-01\n",
      "(2) Errors on training set (1917 samples): \n",
      "min=4.576e-02 / mean=9.052e-02 / median=8.756e-02 / max=1.950e-01\n",
      "(2) Errors on validation set (107 samples): \n",
      "min=5.596e-02 / mean=2.271e-01 / median=2.008e-01 / max=7.073e-01\n",
      "========= 1 / 25 =========\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "sizes_ll = range(1, sizes[-1]+1)\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 2\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize features of each sample\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "\n",
    "ae.investigateLastLayerTrain(T, train_indices[:n_splits], test_indices[:n_splits], sizes, sizes_ll,\n",
    "        model_dirname, nEpoch, batch_size, num_workers, model_filename_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying last layer size: approximation error\n",
    "\n",
    "In this code data encoded and decoded with previously trained models. Resulting approximation (relative residual error by means of  l1/l2  norm) is printed sample-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll=1_model_ae_0 [1600, 400, 100, 1]\n",
      "min=1.956e-01 / mean=4.959e-01 / median=4.828e-01 / max=1.012e+00\n",
      "min=1.945e-01 / mean=5.290e-01 / median=4.840e-01 / max=1.124e+00\n",
      "ll=1_model_ae_1 [1600, 400, 100, 1]\n",
      "min=1.779e-01 / mean=4.931e-01 / median=4.729e-01 / max=1.029e+00\n",
      "min=1.969e-01 / mean=4.885e-01 / median=4.604e-01 / max=1.048e+00\n",
      "ll=1_model_ae_2 [1600, 400, 100, 1]\n",
      "min=1.753e-01 / mean=4.867e-01 / median=4.686e-01 / max=1.081e+00\n",
      "min=2.070e-01 / mean=5.321e-01 / median=4.819e-01 / max=1.214e+00\n",
      "ll=1_model_ae_3 [1600, 400, 100, 1]\n",
      "min=1.429e-01 / mean=4.804e-01 / median=4.620e-01 / max=1.162e+00\n",
      "min=1.778e-01 / mean=5.350e-01 / median=5.059e-01 / max=1.117e+00\n",
      "ll=1_model_ae_4 [1600, 400, 100, 1]\n",
      "min=1.765e-01 / mean=4.887e-01 / median=4.689e-01 / max=1.046e+00\n",
      "min=1.894e-01 / mean=5.182e-01 / median=4.969e-01 / max=1.205e+00\n",
      "ll=2_model_ae_0 [1600, 400, 100, 2]\n",
      "min=1.121e-01 / mean=3.802e-01 / median=3.629e-01 / max=8.718e-01\n",
      "min=1.326e-01 / mean=5.024e-01 / median=4.598e-01 / max=1.830e+00\n",
      "ll=2_model_ae_1 [1600, 400, 100, 2]\n",
      "min=1.123e-01 / mean=3.664e-01 / median=3.470e-01 / max=8.396e-01\n",
      "min=1.362e-01 / mean=4.574e-01 / median=4.245e-01 / max=1.462e+00\n",
      "ll=2_model_ae_2 [1600, 400, 100, 2]\n",
      "min=1.191e-01 / mean=3.639e-01 / median=3.431e-01 / max=9.434e-01\n",
      "min=1.428e-01 / mean=4.894e-01 / median=4.474e-01 / max=1.061e+00\n",
      "ll=2_model_ae_3 [1600, 400, 100, 2]\n",
      "min=1.037e-01 / mean=3.568e-01 / median=3.368e-01 / max=9.742e-01\n",
      "min=1.485e-01 / mean=4.960e-01 / median=4.412e-01 / max=1.279e+00\n",
      "ll=2_model_ae_4 [1600, 400, 100, 2]\n",
      "min=1.073e-01 / mean=3.687e-01 / median=3.422e-01 / max=8.884e-01\n",
      "min=1.102e-01 / mean=4.611e-01 / median=4.233e-01 / max=1.152e+00\n",
      "ll=3_model_ae_0 [1600, 400, 100, 3]\n",
      "min=9.892e-02 / mean=2.938e-01 / median=2.766e-01 / max=7.760e-01\n",
      "min=1.226e-01 / mean=4.624e-01 / median=4.090e-01 / max=2.062e+00\n",
      "ll=3_model_ae_1 [1600, 400, 100, 3]\n",
      "min=8.470e-02 / mean=2.945e-01 / median=2.764e-01 / max=9.572e-01\n",
      "min=1.226e-01 / mean=4.105e-01 / median=3.570e-01 / max=1.194e+00\n",
      "ll=3_model_ae_2 [1600, 400, 100, 3]\n",
      "min=7.283e-02 / mean=2.774e-01 / median=2.520e-01 / max=7.552e-01\n",
      "min=1.040e-01 / mean=4.403e-01 / median=3.805e-01 / max=1.353e+00\n",
      "ll=3_model_ae_3 [1600, 400, 100, 3]\n",
      "min=9.257e-02 / mean=2.866e-01 / median=2.680e-01 / max=8.165e-01\n",
      "min=1.341e-01 / mean=4.385e-01 / median=3.881e-01 / max=9.573e-01\n",
      "ll=3_model_ae_4 [1600, 400, 100, 3]\n",
      "min=9.425e-02 / mean=2.735e-01 / median=2.560e-01 / max=7.558e-01\n",
      "min=1.230e-01 / mean=4.317e-01 / median=3.756e-01 / max=1.885e+00\n",
      "ll=4_model_ae_0 [1600, 400, 100, 4]\n",
      "min=8.124e-02 / mean=2.602e-01 / median=2.474e-01 / max=7.620e-01\n",
      "min=1.286e-01 / mean=4.513e-01 / median=3.897e-01 / max=1.253e+00\n",
      "ll=4_model_ae_1 [1600, 400, 100, 4]\n",
      "min=7.617e-02 / mean=2.612e-01 / median=2.464e-01 / max=7.401e-01\n",
      "min=1.160e-01 / mean=3.905e-01 / median=3.372e-01 / max=2.225e+00\n",
      "ll=4_model_ae_2 [1600, 400, 100, 4]\n",
      "min=9.081e-02 / mean=2.462e-01 / median=2.280e-01 / max=9.134e-01\n",
      "min=9.097e-02 / mean=4.180e-01 / median=3.626e-01 / max=1.154e+00\n",
      "ll=4_model_ae_3 [1600, 400, 100, 4]\n",
      "min=7.522e-02 / mean=2.557e-01 / median=2.395e-01 / max=7.049e-01\n",
      "min=1.279e-01 / mean=4.418e-01 / median=3.716e-01 / max=1.312e+00\n",
      "ll=4_model_ae_4 [1600, 400, 100, 4]\n",
      "min=8.826e-02 / mean=2.484e-01 / median=2.337e-01 / max=6.217e-01\n",
      "min=1.039e-01 / mean=3.954e-01 / median=3.281e-01 / max=1.162e+00\n",
      "ll=5_model_ae_0 [1600, 400, 100, 5]\n",
      "min=8.473e-02 / mean=2.314e-01 / median=2.162e-01 / max=6.700e-01\n",
      "min=1.228e-01 / mean=4.164e-01 / median=3.890e-01 / max=1.218e+00\n",
      "ll=5_model_ae_1 [1600, 400, 100, 5]\n",
      "min=8.357e-02 / mean=2.370e-01 / median=2.202e-01 / max=7.438e-01\n",
      "min=1.234e-01 / mean=3.838e-01 / median=3.137e-01 / max=1.417e+00\n",
      "ll=5_model_ae_2 [1600, 400, 100, 5]\n",
      "min=8.603e-02 / mean=2.344e-01 / median=2.129e-01 / max=6.935e-01\n",
      "min=9.600e-02 / mean=4.042e-01 / median=3.360e-01 / max=1.112e+00\n",
      "ll=5_model_ae_3 [1600, 400, 100, 5]\n",
      "min=8.036e-02 / mean=2.337e-01 / median=2.191e-01 / max=7.400e-01\n",
      "min=1.124e-01 / mean=4.083e-01 / median=3.414e-01 / max=1.067e+00\n",
      "ll=5_model_ae_4 [1600, 400, 100, 5]\n",
      "min=8.145e-02 / mean=2.250e-01 / median=2.124e-01 / max=6.892e-01\n",
      "min=9.264e-02 / mean=3.748e-01 / median=3.170e-01 / max=9.485e-01\n",
      "ll=6_model_ae_0 [1600, 400, 100, 6]\n",
      "min=7.899e-02 / mean=2.149e-01 / median=2.028e-01 / max=6.263e-01\n",
      "min=1.186e-01 / mean=4.029e-01 / median=3.538e-01 / max=1.024e+00\n",
      "ll=6_model_ae_1 [1600, 400, 100, 6]\n",
      "min=8.066e-02 / mean=2.195e-01 / median=2.028e-01 / max=7.033e-01\n",
      "min=1.139e-01 / mean=3.641e-01 / median=3.016e-01 / max=1.625e+00\n",
      "ll=6_model_ae_2 [1600, 400, 100, 6]\n",
      "min=6.928e-02 / mean=2.119e-01 / median=1.980e-01 / max=6.310e-01\n",
      "min=9.881e-02 / mean=3.857e-01 / median=3.133e-01 / max=1.234e+00\n",
      "ll=6_model_ae_3 [1600, 400, 100, 6]\n",
      "min=7.284e-02 / mean=2.173e-01 / median=2.049e-01 / max=6.720e-01\n",
      "min=1.201e-01 / mean=3.958e-01 / median=3.340e-01 / max=1.104e+00\n",
      "ll=6_model_ae_4 [1600, 400, 100, 6]\n",
      "min=8.334e-02 / mean=2.092e-01 / median=1.932e-01 / max=6.015e-01\n",
      "min=1.025e-01 / mean=3.538e-01 / median=3.030e-01 / max=1.044e+00\n",
      "ll=7_model_ae_0 [1600, 400, 100, 7]\n",
      "min=7.451e-02 / mean=1.983e-01 / median=1.843e-01 / max=6.542e-01\n",
      "min=1.168e-01 / mean=3.955e-01 / median=3.318e-01 / max=1.157e+00\n",
      "ll=7_model_ae_1 [1600, 400, 100, 7]\n",
      "min=6.767e-02 / mean=2.075e-01 / median=1.951e-01 / max=6.529e-01\n",
      "min=9.795e-02 / mean=3.394e-01 / median=3.048e-01 / max=1.052e+00\n",
      "ll=7_model_ae_2 [1600, 400, 100, 7]\n",
      "min=7.715e-02 / mean=1.984e-01 / median=1.835e-01 / max=6.821e-01\n",
      "min=8.348e-02 / mean=3.709e-01 / median=2.978e-01 / max=8.536e-01\n",
      "ll=7_model_ae_3 [1600, 400, 100, 7]\n",
      "min=6.990e-02 / mean=1.935e-01 / median=1.794e-01 / max=5.623e-01\n",
      "min=9.699e-02 / mean=3.796e-01 / median=3.147e-01 / max=1.067e+00\n",
      "ll=7_model_ae_4 [1600, 400, 100, 7]\n",
      "min=8.272e-02 / mean=1.910e-01 / median=1.792e-01 / max=6.169e-01\n",
      "min=1.047e-01 / mean=3.535e-01 / median=3.089e-01 / max=9.893e-01\n",
      "ll=8_model_ae_0 [1600, 400, 100, 8]\n",
      "min=7.865e-02 / mean=1.912e-01 / median=1.776e-01 / max=6.048e-01\n",
      "min=1.069e-01 / mean=3.954e-01 / median=3.407e-01 / max=1.315e+00\n",
      "ll=8_model_ae_1 [1600, 400, 100, 8]\n",
      "min=6.851e-02 / mean=1.875e-01 / median=1.780e-01 / max=6.138e-01\n",
      "min=9.153e-02 / mean=3.260e-01 / median=2.809e-01 / max=9.391e-01\n",
      "ll=8_model_ae_2 [1600, 400, 100, 8]\n",
      "min=7.110e-02 / mean=1.789e-01 / median=1.670e-01 / max=5.734e-01\n",
      "min=7.635e-02 / mean=3.502e-01 / median=2.817e-01 / max=8.578e-01\n",
      "ll=8_model_ae_3 [1600, 400, 100, 8]\n",
      "min=7.041e-02 / mean=1.813e-01 / median=1.707e-01 / max=5.830e-01\n",
      "min=9.711e-02 / mean=3.704e-01 / median=3.145e-01 / max=1.006e+00\n",
      "ll=8_model_ae_4 [1600, 400, 100, 8]\n",
      "min=8.179e-02 / mean=1.872e-01 / median=1.745e-01 / max=5.747e-01\n",
      "min=1.085e-01 / mean=3.511e-01 / median=2.967e-01 / max=1.096e+00\n",
      "ll=9_model_ae_0 [1600, 400, 100, 9]\n",
      "min=7.133e-02 / mean=1.747e-01 / median=1.629e-01 / max=6.238e-01\n",
      "min=9.025e-02 / mean=3.829e-01 / median=3.443e-01 / max=1.391e+00\n",
      "ll=9_model_ae_1 [1600, 400, 100, 9]\n",
      "min=6.195e-02 / mean=1.747e-01 / median=1.644e-01 / max=6.125e-01\n",
      "min=9.905e-02 / mean=3.203e-01 / median=2.724e-01 / max=1.078e+00\n",
      "ll=9_model_ae_2 [1600, 400, 100, 9]\n",
      "min=6.265e-02 / mean=1.693e-01 / median=1.574e-01 / max=6.162e-01\n",
      "min=6.896e-02 / mean=3.409e-01 / median=2.829e-01 / max=9.909e-01\n",
      "ll=9_model_ae_3 [1600, 400, 100, 9]\n",
      "min=6.752e-02 / mean=1.733e-01 / median=1.646e-01 / max=5.779e-01\n",
      "min=9.273e-02 / mean=3.446e-01 / median=2.798e-01 / max=1.026e+00\n",
      "ll=9_model_ae_4 [1600, 400, 100, 9]\n",
      "min=7.411e-02 / mean=1.738e-01 / median=1.624e-01 / max=4.987e-01\n",
      "min=8.168e-02 / mean=3.208e-01 / median=2.753e-01 / max=1.118e+00\n",
      "ll=10_model_ae_0 [1600, 400, 100, 10]\n",
      "min=7.195e-02 / mean=1.639e-01 / median=1.553e-01 / max=5.679e-01\n",
      "min=7.932e-02 / mean=3.676e-01 / median=3.135e-01 / max=1.191e+00\n",
      "ll=10_model_ae_1 [1600, 400, 100, 10]\n",
      "min=6.275e-02 / mean=1.674e-01 / median=1.589e-01 / max=6.101e-01\n",
      "min=9.550e-02 / mean=3.133e-01 / median=2.581e-01 / max=1.241e+00\n",
      "ll=10_model_ae_2 [1600, 400, 100, 10]\n",
      "min=6.911e-02 / mean=1.560e-01 / median=1.475e-01 / max=6.063e-01\n",
      "min=7.363e-02 / mean=3.273e-01 / median=2.615e-01 / max=1.028e+00\n",
      "ll=10_model_ae_3 [1600, 400, 100, 10]\n",
      "min=6.193e-02 / mean=1.619e-01 / median=1.542e-01 / max=5.834e-01\n",
      "min=8.689e-02 / mean=3.310e-01 / median=2.734e-01 / max=7.884e-01\n",
      "ll=10_model_ae_4 [1600, 400, 100, 10]\n",
      "min=6.199e-02 / mean=1.578e-01 / median=1.488e-01 / max=5.121e-01\n",
      "min=7.108e-02 / mean=3.070e-01 / median=2.504e-01 / max=8.360e-01\n",
      "ll=11_model_ae_0 [1600, 400, 100, 11]\n",
      "min=7.043e-02 / mean=1.532e-01 / median=1.451e-01 / max=6.401e-01\n",
      "min=6.608e-02 / mean=3.626e-01 / median=3.203e-01 / max=8.699e-01\n",
      "ll=11_model_ae_1 [1600, 400, 100, 11]\n",
      "min=6.591e-02 / mean=1.642e-01 / median=1.544e-01 / max=5.965e-01\n",
      "min=9.634e-02 / mean=2.993e-01 / median=2.659e-01 / max=1.146e+00\n",
      "ll=11_model_ae_2 [1600, 400, 100, 11]\n",
      "min=6.040e-02 / mean=1.520e-01 / median=1.443e-01 / max=5.208e-01\n",
      "min=7.725e-02 / mean=3.251e-01 / median=2.687e-01 / max=1.088e+00\n",
      "ll=11_model_ae_3 [1600, 400, 100, 11]\n",
      "min=6.680e-02 / mean=1.566e-01 / median=1.476e-01 / max=5.829e-01\n",
      "min=9.182e-02 / mean=3.266e-01 / median=2.826e-01 / max=8.504e-01\n",
      "ll=11_model_ae_4 [1600, 400, 100, 11]\n",
      "min=6.237e-02 / mean=1.568e-01 / median=1.493e-01 / max=5.111e-01\n",
      "min=7.445e-02 / mean=3.036e-01 / median=2.605e-01 / max=9.818e-01\n",
      "ll=12_model_ae_0 [1600, 400, 100, 12]\n",
      "min=6.618e-02 / mean=1.481e-01 / median=1.411e-01 / max=5.644e-01\n",
      "min=6.642e-02 / mean=3.542e-01 / median=3.069e-01 / max=9.459e-01\n",
      "ll=12_model_ae_1 [1600, 400, 100, 12]\n",
      "min=5.767e-02 / mean=1.494e-01 / median=1.422e-01 / max=6.088e-01\n",
      "min=9.192e-02 / mean=2.933e-01 / median=2.423e-01 / max=9.473e-01\n",
      "ll=12_model_ae_2 [1600, 400, 100, 12]\n",
      "min=6.454e-02 / mean=1.466e-01 / median=1.387e-01 / max=5.322e-01\n",
      "min=7.104e-02 / mean=3.183e-01 / median=2.633e-01 / max=1.050e+00\n",
      "ll=12_model_ae_3 [1600, 400, 100, 12]\n",
      "min=7.509e-02 / mean=1.483e-01 / median=1.396e-01 / max=5.646e-01\n",
      "min=8.549e-02 / mean=3.083e-01 / median=2.572e-01 / max=7.881e-01\n",
      "ll=12_model_ae_4 [1600, 400, 100, 12]\n",
      "min=7.704e-02 / mean=1.532e-01 / median=1.452e-01 / max=5.089e-01\n",
      "min=8.100e-02 / mean=2.980e-01 / median=2.360e-01 / max=9.908e-01\n",
      "ll=13_model_ae_0 [1600, 400, 100, 13]\n",
      "min=6.366e-02 / mean=1.454e-01 / median=1.379e-01 / max=4.986e-01\n",
      "min=7.223e-02 / mean=3.349e-01 / median=2.841e-01 / max=9.245e-01\n",
      "ll=13_model_ae_1 [1600, 400, 100, 13]\n",
      "min=6.679e-02 / mean=1.453e-01 / median=1.380e-01 / max=5.122e-01\n",
      "min=8.204e-02 / mean=3.043e-01 / median=2.465e-01 / max=1.351e+00\n",
      "ll=13_model_ae_2 [1600, 400, 100, 13]\n",
      "min=5.854e-02 / mean=1.430e-01 / median=1.334e-01 / max=5.387e-01\n",
      "min=6.612e-02 / mean=3.081e-01 / median=2.522e-01 / max=8.880e-01\n",
      "ll=13_model_ae_3 [1600, 400, 100, 13]\n",
      "min=6.521e-02 / mean=1.435e-01 / median=1.367e-01 / max=4.283e-01\n",
      "min=8.314e-02 / mean=2.994e-01 / median=2.502e-01 / max=7.997e-01\n",
      "ll=13_model_ae_4 [1600, 400, 100, 13]\n",
      "min=5.597e-02 / mean=1.382e-01 / median=1.326e-01 / max=5.056e-01\n",
      "min=7.271e-02 / mean=3.033e-01 / median=2.347e-01 / max=9.813e-01\n",
      "ll=14_model_ae_0 [1600, 400, 100, 14]\n",
      "min=5.996e-02 / mean=1.370e-01 / median=1.296e-01 / max=3.840e-01\n",
      "min=7.318e-02 / mean=3.339e-01 / median=2.842e-01 / max=1.031e+00\n",
      "ll=14_model_ae_1 [1600, 400, 100, 14]\n",
      "min=6.178e-02 / mean=1.415e-01 / median=1.344e-01 / max=4.205e-01\n",
      "min=9.178e-02 / mean=2.872e-01 / median=2.352e-01 / max=1.093e+00\n",
      "ll=14_model_ae_2 [1600, 400, 100, 14]\n",
      "min=6.025e-02 / mean=1.360e-01 / median=1.291e-01 / max=5.325e-01\n",
      "min=7.286e-02 / mean=3.105e-01 / median=2.487e-01 / max=9.253e-01\n",
      "ll=14_model_ae_3 [1600, 400, 100, 14]\n",
      "min=5.957e-02 / mean=1.374e-01 / median=1.308e-01 / max=4.295e-01\n",
      "min=9.410e-02 / mean=3.070e-01 / median=2.487e-01 / max=8.341e-01\n",
      "ll=14_model_ae_4 [1600, 400, 100, 14]\n",
      "min=5.826e-02 / mean=1.360e-01 / median=1.276e-01 / max=4.582e-01\n",
      "min=6.722e-02 / mean=2.929e-01 / median=2.260e-01 / max=8.761e-01\n",
      "ll=15_model_ae_0 [1600, 400, 100, 15]\n",
      "min=5.890e-02 / mean=1.359e-01 / median=1.283e-01 / max=5.647e-01\n",
      "min=5.882e-02 / mean=3.344e-01 / median=2.740e-01 / max=1.026e+00\n",
      "ll=15_model_ae_1 [1600, 400, 100, 15]\n",
      "min=5.502e-02 / mean=1.386e-01 / median=1.331e-01 / max=4.549e-01\n",
      "min=8.802e-02 / mean=2.712e-01 / median=2.277e-01 / max=7.436e-01\n",
      "ll=15_model_ae_2 [1600, 400, 100, 15]\n",
      "min=5.737e-02 / mean=1.323e-01 / median=1.258e-01 / max=5.293e-01\n",
      "min=6.215e-02 / mean=3.010e-01 / median=2.431e-01 / max=8.739e-01\n",
      "ll=15_model_ae_3 [1600, 400, 100, 15]\n",
      "min=6.023e-02 / mean=1.341e-01 / median=1.288e-01 / max=5.374e-01\n",
      "min=8.274e-02 / mean=3.014e-01 / median=2.460e-01 / max=7.821e-01\n",
      "ll=15_model_ae_4 [1600, 400, 100, 15]\n",
      "min=5.810e-02 / mean=1.343e-01 / median=1.281e-01 / max=5.048e-01\n",
      "min=6.454e-02 / mean=2.798e-01 / median=2.253e-01 / max=8.085e-01\n",
      "ll=16_model_ae_0 [1600, 400, 100, 16]\n",
      "min=6.056e-02 / mean=1.304e-01 / median=1.243e-01 / max=3.731e-01\n",
      "min=7.017e-02 / mean=3.205e-01 / median=2.565e-01 / max=1.098e+00\n",
      "ll=16_model_ae_1 [1600, 400, 100, 16]\n",
      "min=5.536e-02 / mean=1.302e-01 / median=1.242e-01 / max=4.008e-01\n",
      "min=8.808e-02 / mean=2.765e-01 / median=2.258e-01 / max=1.021e+00\n",
      "ll=16_model_ae_2 [1600, 400, 100, 16]\n",
      "min=5.907e-02 / mean=1.314e-01 / median=1.239e-01 / max=4.242e-01\n",
      "min=6.085e-02 / mean=2.827e-01 / median=2.301e-01 / max=7.629e-01\n",
      "ll=16_model_ae_3 [1600, 400, 100, 16]\n",
      "min=5.823e-02 / mean=1.320e-01 / median=1.263e-01 / max=3.815e-01\n",
      "min=9.312e-02 / mean=3.061e-01 / median=2.516e-01 / max=8.108e-01\n",
      "ll=16_model_ae_4 [1600, 400, 100, 16]\n",
      "min=5.603e-02 / mean=1.324e-01 / median=1.257e-01 / max=4.409e-01\n",
      "min=6.597e-02 / mean=2.921e-01 / median=2.175e-01 / max=8.589e-01\n",
      "ll=17_model_ae_0 [1600, 400, 100, 17]\n",
      "min=5.956e-02 / mean=1.273e-01 / median=1.222e-01 / max=3.783e-01\n",
      "min=6.615e-02 / mean=3.256e-01 / median=2.795e-01 / max=9.012e-01\n",
      "ll=17_model_ae_1 [1600, 400, 100, 17]\n",
      "min=5.653e-02 / mean=1.307e-01 / median=1.261e-01 / max=3.711e-01\n",
      "min=8.874e-02 / mean=2.685e-01 / median=2.263e-01 / max=8.303e-01\n",
      "ll=17_model_ae_2 [1600, 400, 100, 17]\n",
      "min=5.457e-02 / mean=1.284e-01 / median=1.221e-01 / max=5.300e-01\n",
      "min=5.868e-02 / mean=2.978e-01 / median=2.291e-01 / max=9.542e-01\n",
      "ll=17_model_ae_3 [1600, 400, 100, 17]\n",
      "min=5.287e-02 / mean=1.277e-01 / median=1.224e-01 / max=3.968e-01\n",
      "min=8.136e-02 / mean=2.993e-01 / median=2.480e-01 / max=8.386e-01\n",
      "ll=17_model_ae_4 [1600, 400, 100, 17]\n",
      "min=6.370e-02 / mean=1.316e-01 / median=1.266e-01 / max=4.309e-01\n",
      "min=7.672e-02 / mean=2.762e-01 / median=2.251e-01 / max=7.887e-01\n",
      "ll=18_model_ae_0 [1600, 400, 100, 18]\n",
      "min=5.807e-02 / mean=1.236e-01 / median=1.194e-01 / max=3.650e-01\n",
      "min=6.298e-02 / mean=3.133e-01 / median=2.595e-01 / max=7.964e-01\n",
      "ll=18_model_ae_1 [1600, 400, 100, 18]\n",
      "min=5.568e-02 / mean=1.252e-01 / median=1.206e-01 / max=3.888e-01\n",
      "min=8.386e-02 / mean=2.685e-01 / median=2.206e-01 / max=1.059e+00\n",
      "ll=18_model_ae_2 [1600, 400, 100, 18]\n",
      "min=5.381e-02 / mean=1.223e-01 / median=1.168e-01 / max=3.669e-01\n",
      "min=6.701e-02 / mean=2.930e-01 / median=2.374e-01 / max=1.021e+00\n",
      "ll=18_model_ae_3 [1600, 400, 100, 18]\n",
      "min=6.497e-02 / mean=1.233e-01 / median=1.181e-01 / max=4.215e-01\n",
      "min=8.191e-02 / mean=2.857e-01 / median=2.351e-01 / max=8.757e-01\n",
      "ll=18_model_ae_4 [1600, 400, 100, 18]\n",
      "min=5.149e-02 / mean=1.216e-01 / median=1.155e-01 / max=5.040e-01\n",
      "min=6.039e-02 / mean=2.732e-01 / median=2.217e-01 / max=9.319e-01\n",
      "ll=19_model_ae_0 [1600, 400, 100, 19]\n",
      "min=5.649e-02 / mean=1.230e-01 / median=1.175e-01 / max=5.207e-01\n",
      "min=6.099e-02 / mean=3.206e-01 / median=2.660e-01 / max=8.945e-01\n",
      "ll=19_model_ae_1 [1600, 400, 100, 19]\n",
      "min=5.103e-02 / mean=1.216e-01 / median=1.175e-01 / max=3.542e-01\n",
      "min=7.553e-02 / mean=2.637e-01 / median=2.102e-01 / max=8.013e-01\n",
      "ll=19_model_ae_2 [1600, 400, 100, 19]\n",
      "min=5.152e-02 / mean=1.175e-01 / median=1.113e-01 / max=3.607e-01\n",
      "min=5.880e-02 / mean=2.748e-01 / median=2.218e-01 / max=6.736e-01\n",
      "ll=19_model_ae_3 [1600, 400, 100, 19]\n",
      "min=5.185e-02 / mean=1.191e-01 / median=1.137e-01 / max=4.912e-01\n",
      "min=7.176e-02 / mean=2.773e-01 / median=2.378e-01 / max=7.574e-01\n",
      "ll=19_model_ae_4 [1600, 400, 100, 19]\n",
      "min=5.084e-02 / mean=1.181e-01 / median=1.122e-01 / max=3.179e-01\n",
      "min=6.513e-02 / mean=2.624e-01 / median=2.070e-01 / max=8.070e-01\n",
      "ll=20_model_ae_0 [1600, 400, 100, 20]\n",
      "min=6.516e-02 / mean=1.239e-01 / median=1.180e-01 / max=3.812e-01\n",
      "min=6.832e-02 / mean=3.215e-01 / median=2.705e-01 / max=1.123e+00\n",
      "ll=20_model_ae_1 [1600, 400, 100, 20]\n",
      "min=4.625e-02 / mean=1.179e-01 / median=1.144e-01 / max=3.085e-01\n",
      "min=7.738e-02 / mean=2.553e-01 / median=2.119e-01 / max=8.884e-01\n",
      "ll=20_model_ae_2 [1600, 400, 100, 20]\n",
      "min=5.838e-02 / mean=1.192e-01 / median=1.130e-01 / max=3.607e-01\n",
      "min=6.913e-02 / mean=2.800e-01 / median=2.298e-01 / max=7.009e-01\n",
      "ll=20_model_ae_3 [1600, 400, 100, 20]\n",
      "min=4.944e-02 / mean=1.159e-01 / median=1.106e-01 / max=3.439e-01\n",
      "min=8.130e-02 / mean=2.780e-01 / median=2.377e-01 / max=6.810e-01\n",
      "ll=20_model_ae_4 [1600, 400, 100, 20]\n",
      "min=5.196e-02 / mean=1.175e-01 / median=1.126e-01 / max=5.051e-01\n",
      "min=6.091e-02 / mean=2.749e-01 / median=2.168e-01 / max=8.158e-01\n",
      "ll=21_model_ae_0 [1600, 400, 100, 21]\n",
      "min=6.212e-02 / mean=1.200e-01 / median=1.158e-01 / max=4.466e-01\n",
      "min=8.437e-02 / mean=3.091e-01 / median=2.591e-01 / max=1.066e+00\n",
      "ll=21_model_ae_1 [1600, 400, 100, 21]\n",
      "min=4.903e-02 / mean=1.158e-01 / median=1.114e-01 / max=3.825e-01\n",
      "min=8.278e-02 / mean=2.544e-01 / median=2.039e-01 / max=7.778e-01\n",
      "ll=21_model_ae_2 [1600, 400, 100, 21]\n",
      "min=5.360e-02 / mean=1.163e-01 / median=1.093e-01 / max=5.209e-01\n",
      "min=6.419e-02 / mean=2.798e-01 / median=2.330e-01 / max=9.093e-01\n",
      "ll=21_model_ae_3 [1600, 400, 100, 21]\n",
      "min=5.409e-02 / mean=1.203e-01 / median=1.159e-01 / max=3.305e-01\n",
      "min=8.072e-02 / mean=2.793e-01 / median=2.185e-01 / max=7.234e-01\n",
      "ll=21_model_ae_4 [1600, 400, 100, 21]\n",
      "min=5.638e-02 / mean=1.174e-01 / median=1.137e-01 / max=2.974e-01\n",
      "min=6.630e-02 / mean=2.640e-01 / median=2.121e-01 / max=7.357e-01\n",
      "ll=22_model_ae_0 [1600, 400, 100, 22]\n",
      "min=5.704e-02 / mean=1.188e-01 / median=1.152e-01 / max=3.580e-01\n",
      "min=6.832e-02 / mean=3.082e-01 / median=2.642e-01 / max=1.096e+00\n",
      "ll=22_model_ae_1 [1600, 400, 100, 22]\n",
      "min=5.578e-02 / mean=1.177e-01 / median=1.140e-01 / max=3.207e-01\n",
      "min=7.963e-02 / mean=2.543e-01 / median=2.016e-01 / max=8.468e-01\n",
      "ll=22_model_ae_2 [1600, 400, 100, 22]\n",
      "min=5.058e-02 / mean=1.133e-01 / median=1.078e-01 / max=5.210e-01\n",
      "min=5.570e-02 / mean=2.775e-01 / median=2.170e-01 / max=8.150e-01\n",
      "ll=22_model_ae_3 [1600, 400, 100, 22]\n",
      "min=5.319e-02 / mean=1.160e-01 / median=1.109e-01 / max=3.437e-01\n",
      "min=8.095e-02 / mean=2.785e-01 / median=2.305e-01 / max=7.529e-01\n",
      "ll=22_model_ae_4 [1600, 400, 100, 22]\n",
      "min=5.154e-02 / mean=1.133e-01 / median=1.089e-01 / max=3.335e-01\n",
      "min=6.486e-02 / mean=2.576e-01 / median=2.113e-01 / max=7.564e-01\n",
      "ll=23_model_ae_0 [1600, 400, 100, 23]\n",
      "min=5.658e-02 / mean=1.152e-01 / median=1.108e-01 / max=3.452e-01\n",
      "min=6.283e-02 / mean=3.106e-01 / median=2.584e-01 / max=1.075e+00\n",
      "ll=23_model_ae_1 [1600, 400, 100, 23]\n",
      "min=4.843e-02 / mean=1.129e-01 / median=1.097e-01 / max=2.713e-01\n",
      "min=8.405e-02 / mean=2.520e-01 / median=2.010e-01 / max=8.727e-01\n",
      "ll=23_model_ae_2 [1600, 400, 100, 23]\n",
      "min=4.820e-02 / mean=1.114e-01 / median=1.058e-01 / max=5.171e-01\n",
      "min=5.807e-02 / mean=2.699e-01 / median=2.158e-01 / max=7.228e-01\n",
      "ll=23_model_ae_3 [1600, 400, 100, 23]\n",
      "min=4.986e-02 / mean=1.109e-01 / median=1.056e-01 / max=3.248e-01\n",
      "min=7.656e-02 / mean=2.738e-01 / median=2.259e-01 / max=8.405e-01\n",
      "ll=23_model_ae_4 [1600, 400, 100, 23]\n",
      "min=5.740e-02 / mean=1.126e-01 / median=1.082e-01 / max=2.561e-01\n",
      "min=7.534e-02 / mean=2.609e-01 / median=1.966e-01 / max=8.863e-01\n",
      "ll=24_model_ae_0 [1600, 400, 100, 24]\n",
      "min=4.973e-02 / mean=1.092e-01 / median=1.047e-01 / max=3.519e-01\n",
      "min=5.696e-02 / mean=3.023e-01 / median=2.550e-01 / max=1.074e+00\n",
      "ll=24_model_ae_1 [1600, 400, 100, 24]\n",
      "min=4.672e-02 / mean=1.122e-01 / median=1.083e-01 / max=3.374e-01\n",
      "min=7.613e-02 / mean=2.487e-01 / median=1.989e-01 / max=8.682e-01\n",
      "ll=24_model_ae_2 [1600, 400, 100, 24]\n",
      "min=4.886e-02 / mean=1.068e-01 / median=1.015e-01 / max=5.153e-01\n",
      "min=5.174e-02 / mean=2.681e-01 / median=2.075e-01 / max=9.455e-01\n",
      "ll=24_model_ae_3 [1600, 400, 100, 24]\n",
      "min=5.859e-02 / mean=1.203e-01 / median=1.168e-01 / max=3.267e-01\n",
      "min=8.412e-02 / mean=2.829e-01 / median=2.225e-01 / max=8.361e-01\n",
      "ll=24_model_ae_4 [1600, 400, 100, 24]\n",
      "min=5.103e-02 / mean=1.084e-01 / median=1.047e-01 / max=2.548e-01\n",
      "min=6.283e-02 / mean=2.520e-01 / median=1.968e-01 / max=8.774e-01\n",
      "ll=25_model_ae_0 [1600, 400, 100, 25]\n",
      "min=5.728e-02 / mean=1.105e-01 / median=1.050e-01 / max=3.424e-01\n",
      "min=5.512e-02 / mean=2.977e-01 / median=2.421e-01 / max=8.333e-01\n",
      "ll=25_model_ae_1 [1600, 400, 100, 25]\n",
      "min=6.160e-02 / mean=1.189e-01 / median=1.151e-01 / max=2.991e-01\n",
      "min=8.929e-02 / mean=2.547e-01 / median=2.055e-01 / max=9.955e-01\n",
      "ll=25_model_ae_2 [1600, 400, 100, 25]\n",
      "min=5.071e-02 / mean=1.075e-01 / median=1.024e-01 / max=3.239e-01\n",
      "min=5.627e-02 / mean=2.634e-01 / median=2.229e-01 / max=7.472e-01\n",
      "ll=25_model_ae_3 [1600, 400, 100, 25]\n",
      "min=5.080e-02 / mean=1.091e-01 / median=1.046e-01 / max=3.176e-01\n",
      "min=7.607e-02 / mean=2.681e-01 / median=2.251e-01 / max=7.306e-01\n",
      "ll=25_model_ae_4 [1600, 400, 100, 25]\n",
      "min=4.973e-02 / mean=1.079e-01 / median=1.042e-01 / max=3.127e-01\n",
      "min=6.159e-02 / mean=2.524e-01 / median=1.982e-01 / max=7.716e-01\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_hard_prefix = 'll='\n",
    "model_filename_base = 'model_ae_'\n",
    "\n",
    "results_filename = 'relres_ae_ll'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "sizes_ll = range(1, sizes[-1]+1)\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 2\n",
    "n_splits = 5\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "N = T.shape[1]\n",
    "\n",
    "# optimizer parameters (needed to initialize AE class before parameters loading)\n",
    "learning_rate = 0.0025\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-5\n",
    "optimizer = lambda params: torch.optim.Adam(params, lr=learning_rate, betas=betas, eps=eps)\n",
    "nls = [nn.ReLU()]+[nn.Sigmoid()]*(len(sizes))\n",
    "\n",
    "\n",
    "model_fname_list = os.listdir(model_dirname)\n",
    "model_fname_list = filter(lambda x: x.startswith(model_filename_hard_prefix), model_fname_list)\n",
    "model_fname_list = filter(lambda x: model_filename_base in x, model_fname_list)\n",
    "model_fname_list = sorted(model_fname_list, key=stringSplitByNumbers)\n",
    "\n",
    "relres_train = []\n",
    "relres_test = []\n",
    "for i_sz in xrange(len(sizes_ll)):\n",
    "    #current_size = sizes_ll[i_sz]\n",
    "    sizes_local = [N] + sizes[:-1] + [sizes_ll[i_sz]]\n",
    "    autoencoder = ae.AutoEncoder(sizes_local, nls, optimizer=optimizer, loss=nn.SmoothL1Loss)\n",
    "    relres_train_loc = []\n",
    "    relres_test_loc = []\n",
    "    for i_cv in xrange(n_splits):\n",
    "        num_model = i_sz*n_splits + i_cv\n",
    "        model_fname = model_fname_list[num_model]\n",
    "        print model_fname, sizes_local\n",
    "        autoencoder.load_state_dict(torch.load(model_dirname+model_fname))\n",
    "        _, tmp = ae.getStats(autoencoder, T[train_indices[i_cv]])\n",
    "        relres_train_loc.append(np.median(tmp))\n",
    "        _, tmp = ae.getStats(autoencoder, T[test_indices[i_cv]])\n",
    "        relres_test_loc.append(np.median(tmp))\n",
    "    relres_train.append(np.median(relres_train_loc))\n",
    "    relres_test.append(np.median(relres_test_loc))\n",
    "np.savez_compressed(model_dirname+results_filename, relres_train=relres_train, relres_test=relres_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: encode data\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded with ll=1_model_ae_0. Comp.time=0.23442\n",
      "encoded with ll=1_model_ae_1. Comp.time=0.23787\n",
      "encoded with ll=1_model_ae_2. Comp.time=0.23444\n",
      "encoded with ll=1_model_ae_3. Comp.time=0.23111\n",
      "encoded with ll=1_model_ae_4. Comp.time=0.23116\n",
      "../models/autoencoder/last_layer/ll=1_model_ae_\n",
      "encoded with ll=2_model_ae_0. Comp.time=0.23084\n",
      "encoded with ll=2_model_ae_1. Comp.time=0.23157\n",
      "encoded with ll=2_model_ae_2. Comp.time=0.23165\n",
      "encoded with ll=2_model_ae_3. Comp.time=0.23075\n",
      "encoded with ll=2_model_ae_4. Comp.time=0.23135\n",
      "../models/autoencoder/last_layer/ll=2_model_ae_\n",
      "encoded with ll=3_model_ae_0. Comp.time=0.23144\n",
      "encoded with ll=3_model_ae_1. Comp.time=0.23334\n",
      "encoded with ll=3_model_ae_2. Comp.time=0.24248\n",
      "encoded with ll=3_model_ae_3. Comp.time=0.23241\n",
      "encoded with ll=3_model_ae_4. Comp.time=0.23321\n",
      "../models/autoencoder/last_layer/ll=3_model_ae_\n",
      "encoded with ll=4_model_ae_0. Comp.time=0.23172\n",
      "encoded with ll=4_model_ae_1. Comp.time=0.23176\n",
      "encoded with ll=4_model_ae_2. Comp.time=0.23169\n",
      "encoded with ll=4_model_ae_3. Comp.time=0.23194\n",
      "encoded with ll=4_model_ae_4. Comp.time=0.23187\n",
      "../models/autoencoder/last_layer/ll=4_model_ae_\n",
      "encoded with ll=5_model_ae_0. Comp.time=0.23222\n",
      "encoded with ll=5_model_ae_1. Comp.time=0.23265\n",
      "encoded with ll=5_model_ae_2. Comp.time=0.23261\n",
      "encoded with ll=5_model_ae_3. Comp.time=0.23317\n",
      "encoded with ll=5_model_ae_4. Comp.time=0.23273\n",
      "../models/autoencoder/last_layer/ll=5_model_ae_\n",
      "encoded with ll=6_model_ae_0. Comp.time=0.23332\n",
      "encoded with ll=6_model_ae_1. Comp.time=0.23290\n",
      "encoded with ll=6_model_ae_2. Comp.time=0.23377\n",
      "encoded with ll=6_model_ae_3. Comp.time=0.23319\n",
      "encoded with ll=6_model_ae_4. Comp.time=0.23345\n",
      "../models/autoencoder/last_layer/ll=6_model_ae_\n",
      "encoded with ll=7_model_ae_0. Comp.time=0.23346\n",
      "encoded with ll=7_model_ae_1. Comp.time=0.23359\n",
      "encoded with ll=7_model_ae_2. Comp.time=0.23422\n",
      "encoded with ll=7_model_ae_3. Comp.time=0.23436\n",
      "encoded with ll=7_model_ae_4. Comp.time=0.23438\n",
      "../models/autoencoder/last_layer/ll=7_model_ae_\n",
      "encoded with ll=8_model_ae_0. Comp.time=0.23373\n",
      "encoded with ll=8_model_ae_1. Comp.time=0.23497\n",
      "encoded with ll=8_model_ae_2. Comp.time=0.23797\n",
      "encoded with ll=8_model_ae_3. Comp.time=0.23464\n",
      "encoded with ll=8_model_ae_4. Comp.time=0.23542\n",
      "../models/autoencoder/last_layer/ll=8_model_ae_\n",
      "encoded with ll=9_model_ae_0. Comp.time=0.23353\n",
      "encoded with ll=9_model_ae_1. Comp.time=0.23378\n",
      "encoded with ll=9_model_ae_2. Comp.time=0.23423\n",
      "encoded with ll=9_model_ae_3. Comp.time=0.23431\n",
      "encoded with ll=9_model_ae_4. Comp.time=0.23485\n",
      "../models/autoencoder/last_layer/ll=9_model_ae_\n",
      "encoded with ll=10_model_ae_0. Comp.time=0.23415\n",
      "encoded with ll=10_model_ae_1. Comp.time=0.23569\n",
      "encoded with ll=10_model_ae_2. Comp.time=0.23668\n",
      "encoded with ll=10_model_ae_3. Comp.time=0.23680\n",
      "encoded with ll=10_model_ae_4. Comp.time=0.23841\n",
      "../models/autoencoder/last_layer/ll=10_model_ae_\n",
      "encoded with ll=11_model_ae_0. Comp.time=0.23556\n",
      "encoded with ll=11_model_ae_1. Comp.time=0.23674\n",
      "encoded with ll=11_model_ae_2. Comp.time=0.23640\n",
      "encoded with ll=11_model_ae_3. Comp.time=0.23630\n",
      "encoded with ll=11_model_ae_4. Comp.time=0.23600\n",
      "../models/autoencoder/last_layer/ll=11_model_ae_\n",
      "encoded with ll=12_model_ae_0. Comp.time=0.23526\n",
      "encoded with ll=12_model_ae_1. Comp.time=0.23557\n",
      "encoded with ll=12_model_ae_2. Comp.time=0.23632\n",
      "encoded with ll=12_model_ae_3. Comp.time=0.23625\n",
      "encoded with ll=12_model_ae_4. Comp.time=0.23639\n",
      "../models/autoencoder/last_layer/ll=12_model_ae_\n",
      "encoded with ll=13_model_ae_0. Comp.time=0.23597\n",
      "encoded with ll=13_model_ae_1. Comp.time=0.23657\n",
      "encoded with ll=13_model_ae_2. Comp.time=0.23673\n",
      "encoded with ll=13_model_ae_3. Comp.time=0.23653\n",
      "encoded with ll=13_model_ae_4. Comp.time=0.23636\n",
      "../models/autoencoder/last_layer/ll=13_model_ae_\n",
      "encoded with ll=14_model_ae_0. Comp.time=0.23645\n",
      "encoded with ll=14_model_ae_1. Comp.time=0.23620\n",
      "encoded with ll=14_model_ae_2. Comp.time=0.23643\n",
      "encoded with ll=14_model_ae_3. Comp.time=0.23673\n",
      "encoded with ll=14_model_ae_4. Comp.time=0.23684\n",
      "../models/autoencoder/last_layer/ll=14_model_ae_\n",
      "encoded with ll=15_model_ae_0. Comp.time=0.23728\n",
      "encoded with ll=15_model_ae_1. Comp.time=0.23777\n",
      "encoded with ll=15_model_ae_2. Comp.time=0.23864\n",
      "encoded with ll=15_model_ae_3. Comp.time=0.23750\n",
      "encoded with ll=15_model_ae_4. Comp.time=0.23935\n",
      "../models/autoencoder/last_layer/ll=15_model_ae_\n",
      "encoded with ll=16_model_ae_0. Comp.time=0.23585\n",
      "encoded with ll=16_model_ae_1. Comp.time=0.23559\n",
      "encoded with ll=16_model_ae_2. Comp.time=0.24020\n",
      "encoded with ll=16_model_ae_3. Comp.time=0.23614\n",
      "encoded with ll=16_model_ae_4. Comp.time=0.23569\n",
      "../models/autoencoder/last_layer/ll=16_model_ae_\n",
      "encoded with ll=17_model_ae_0. Comp.time=0.23730\n",
      "encoded with ll=17_model_ae_1. Comp.time=0.23768\n",
      "encoded with ll=17_model_ae_2. Comp.time=0.23846\n",
      "encoded with ll=17_model_ae_3. Comp.time=0.23762\n",
      "encoded with ll=17_model_ae_4. Comp.time=0.23720\n",
      "../models/autoencoder/last_layer/ll=17_model_ae_\n",
      "encoded with ll=18_model_ae_0. Comp.time=0.23852\n",
      "encoded with ll=18_model_ae_1. Comp.time=0.23780\n",
      "encoded with ll=18_model_ae_2. Comp.time=0.23784\n",
      "encoded with ll=18_model_ae_3. Comp.time=0.23749\n",
      "encoded with ll=18_model_ae_4. Comp.time=0.23842\n",
      "../models/autoencoder/last_layer/ll=18_model_ae_\n",
      "encoded with ll=19_model_ae_0. Comp.time=0.23826\n",
      "encoded with ll=19_model_ae_1. Comp.time=0.23857\n",
      "encoded with ll=19_model_ae_2. Comp.time=0.23883\n",
      "encoded with ll=19_model_ae_3. Comp.time=0.23867\n",
      "encoded with ll=19_model_ae_4. Comp.time=0.23876\n",
      "../models/autoencoder/last_layer/ll=19_model_ae_\n",
      "encoded with ll=20_model_ae_0. Comp.time=0.23818\n",
      "encoded with ll=20_model_ae_1. Comp.time=0.23914\n",
      "encoded with ll=20_model_ae_2. Comp.time=0.23805\n",
      "encoded with ll=20_model_ae_3. Comp.time=0.23786\n",
      "encoded with ll=20_model_ae_4. Comp.time=0.23882\n",
      "../models/autoencoder/last_layer/ll=20_model_ae_\n",
      "encoded with ll=21_model_ae_0. Comp.time=0.23949\n",
      "encoded with ll=21_model_ae_1. Comp.time=0.23924\n",
      "encoded with ll=21_model_ae_2. Comp.time=0.23903\n",
      "encoded with ll=21_model_ae_3. Comp.time=0.23854\n",
      "encoded with ll=21_model_ae_4. Comp.time=0.23965\n",
      "../models/autoencoder/last_layer/ll=21_model_ae_\n",
      "encoded with ll=22_model_ae_0. Comp.time=0.23925\n",
      "encoded with ll=22_model_ae_1. Comp.time=0.23972\n",
      "encoded with ll=22_model_ae_2. Comp.time=0.24176\n",
      "encoded with ll=22_model_ae_3. Comp.time=0.23983\n",
      "encoded with ll=22_model_ae_4. Comp.time=0.23999\n",
      "../models/autoencoder/last_layer/ll=22_model_ae_\n",
      "encoded with ll=23_model_ae_0. Comp.time=0.23956\n",
      "encoded with ll=23_model_ae_1. Comp.time=0.23950\n",
      "encoded with ll=23_model_ae_2. Comp.time=0.24187\n",
      "encoded with ll=23_model_ae_3. Comp.time=0.24008\n",
      "encoded with ll=23_model_ae_4. Comp.time=0.23977\n",
      "../models/autoencoder/last_layer/ll=23_model_ae_\n",
      "encoded with ll=24_model_ae_0. Comp.time=0.23972\n",
      "encoded with ll=24_model_ae_1. Comp.time=0.23932\n",
      "encoded with ll=24_model_ae_2. Comp.time=0.24076\n",
      "encoded with ll=24_model_ae_3. Comp.time=0.24004\n",
      "encoded with ll=24_model_ae_4. Comp.time=0.24154\n",
      "../models/autoencoder/last_layer/ll=24_model_ae_\n",
      "encoded with ll=25_model_ae_0. Comp.time=0.24086\n",
      "encoded with ll=25_model_ae_1. Comp.time=0.24028\n",
      "encoded with ll=25_model_ae_2. Comp.time=0.24234\n",
      "encoded with ll=25_model_ae_3. Comp.time=0.24117\n",
      "encoded with ll=25_model_ae_4. Comp.time=0.24207\n",
      "../models/autoencoder/last_layer/ll=25_model_ae_\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_prefix = 'll='\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset = 'dataset.npz'\n",
    "save_filename_postfix = 'autoencoded_ll_' + filename_dataset\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "num_workers = 2\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "\n",
    "for k in xrange(sizes[-1]):\n",
    "    sizes_ll = sizes[:-1] + [k+1]\n",
    "    model_full_prefix = model_prefix + str(k+1) + '_' + model_filename_prefix\n",
    "    save_filename = model_prefix+str(k+1)+'_'+save_filename_postfix\n",
    "    ae.encodeDataset(\n",
    "        df, sizes_ll, model_dirname, model_full_prefix,\n",
    "        data_dirname+save_filename, num_workers, return_result=0\n",
    "    )\n",
    "    print model_dirname+model_full_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: predict with logistic regression\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hariyuki/apd/lib/python2.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hariyuki/apd/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer size=2\n",
      "last layer size=3\n",
      "last layer size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hariyuki/apd/lib/python2.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer size=5\n",
      "last layer size=6\n",
      "last layer size=7\n",
      "last layer size=8\n",
      "last layer size=9\n",
      "last layer size=10\n",
      "last layer size=11\n",
      "last layer size=12\n",
      "last layer size=13\n",
      "last layer size=14\n",
      "last layer size=15\n",
      "last layer size=16\n",
      "last layer size=17\n",
      "last layer size=18\n",
      "last layer size=19\n",
      "last layer size=20\n",
      "last layer size=21\n",
      "last layer size=22\n",
      "last layer size=23\n",
      "last layer size=24\n",
      "last layer size=25\n",
      "accuracies\n",
      "[[0.12107367 0.12781955]\n",
      " [0.1264637  0.12337662]\n",
      " [0.17247287 0.15037594]\n",
      " [0.25352113 0.14953271]\n",
      " [0.37706667 0.24137931]\n",
      " [0.52738654 0.34579439]\n",
      " [0.6672     0.47663551]\n",
      " [0.73656755 0.47787611]\n",
      " [0.83894917 0.59398496]\n",
      " [0.86293547 0.6137931 ]\n",
      " [0.91096272 0.69172932]\n",
      " [0.93318104 0.66896552]\n",
      " [0.95784543 0.68224299]\n",
      " [0.96630497 0.73793103]\n",
      " [0.97161937 0.75172414]\n",
      " [0.98330551 0.73451327]\n",
      " [0.98720089 0.77876106]\n",
      " [0.98942682 0.7699115 ]\n",
      " [0.99443517 0.80530973]\n",
      " [0.99648712 0.81308411]\n",
      " [0.99648712 0.82068966]\n",
      " [0.99824356 0.81168831]\n",
      " [0.99733333 0.85981308]\n",
      " [0.99843505 0.8411215 ]\n",
      " [0.99824356 0.84137931]]\n",
      "f1 measure\n",
      "[[0.02788155 0.03126521]\n",
      " [0.0402193  0.0362217 ]\n",
      " [0.08511826 0.06173296]\n",
      " [0.19812087 0.08686081]\n",
      " [0.33370047 0.20412151]\n",
      " [0.5060822  0.30047471]\n",
      " [0.65639345 0.44307966]\n",
      " [0.73144748 0.45301972]\n",
      " [0.83671805 0.56881358]\n",
      " [0.86108373 0.58891698]\n",
      " [0.91020162 0.67017544]\n",
      " [0.92950254 0.65865185]\n",
      " [0.9572594  0.67040498]\n",
      " [0.96622075 0.72686118]\n",
      " [0.97128965 0.73253273]\n",
      " [0.98325966 0.72926339]\n",
      " [0.98712566 0.75992415]\n",
      " [0.98920681 0.75073746]\n",
      " [0.99439467 0.79715668]\n",
      " [0.99644228 0.77994363]\n",
      " [0.99647303 0.79339017]\n",
      " [0.99823978 0.80210491]\n",
      " [0.99730032 0.83587005]\n",
      " [0.99841789 0.82956279]\n",
      " [0.99823223 0.83508426]]\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_prefix = 'll='\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset_base = 'autoencoded_ll_dataset.npz'\n",
    "\n",
    "ll_max = 25\n",
    "n_splits = 5\n",
    "\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'll_autoencoder+LR'\n",
    "\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "\n",
    "\n",
    "tms = []\n",
    "#predict_train_all = []\n",
    "#predict_test_all = []\n",
    "\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for l in xrange(ll_max):\n",
    "    print 'last layer size=%d' % (l+1)\n",
    "    filename_data = model_prefix+str(l+1)+'_'+filename_dataset_base\n",
    "    df = np.load(data_dirname+filename_data)\n",
    "    X, y = df['data'], df['label']\n",
    "    y = reshape(y, [-1, 1])\n",
    "    \n",
    "    tms_l = []\n",
    "    predict_train_l = []\n",
    "    predict_test_l = []\n",
    "\n",
    "    accuracies_l = []\n",
    "    f1s_l = []\n",
    "    for k in xrange(n_splits):\n",
    "        train_index = train_indices[k]\n",
    "        test_index = test_indices[k]\n",
    "\n",
    "        classifier = LogisticRegression(\n",
    "            penalty='l1', dual=False, tol=0.0001, C=1000.0, fit_intercept=True,\n",
    "            intercept_scaling=1, class_weight=None, random_state=None,\n",
    "            solver='saga', max_iter=1000, multi_class='multinomial', verbose=0,\n",
    "            warm_start=False, n_jobs=1\n",
    "        )\n",
    "\n",
    "        tic = time.clock();\n",
    "        classifier.fit(X[k][train_index], y[train_index])\n",
    "        toc = time.clock();\n",
    "\n",
    "        tms_loc = [toc-tic]\n",
    "\n",
    "        tic = time.clock()\n",
    "        predict_train = classifier.predict(X[k][train_index])\n",
    "        toc = time.clock()\n",
    "        tms_loc.append(toc-tic)\n",
    "        acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "        f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "        tic = time.clock()\n",
    "        predict_test = classifier.predict(X[k][test_index])\n",
    "        toc = time.clock()\n",
    "        acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "        f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "        #confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "        tms_loc.append(toc-tic)\n",
    "\n",
    "        accuracies_l.append(acc_loc)\n",
    "        f1s_l.append(f1_loc)\n",
    "        tms_l.append(tms_loc)\n",
    "        #predict_train_l.append( predict_train )\n",
    "        #predict_test_l.append( predict_test )\n",
    "        \n",
    "    tms.append(tms_l)\n",
    "    #predict_train_all.append(predict_train_l)\n",
    "    #predict_test_all.append(predict_test_l)\n",
    "\n",
    "    accuracies.append(accuracies_l)\n",
    "    f1s.append(f1s_l)\n",
    "np.savez_compressed(\n",
    "    dirname_results+filename_results, tms=tms,\n",
    "    acc=accuracies, f1=f1s\n",
    ")    \n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=1)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "[[0.12107367 0.12781955]\n",
      " [0.1264637  0.12337662]\n",
      " [0.17247287 0.15037594]\n",
      " [0.25352113 0.14953271]\n",
      " [0.37706667 0.24137931]\n",
      " [0.52738654 0.34579439]\n",
      " [0.6672     0.47663551]\n",
      " [0.73656755 0.47787611]\n",
      " [0.83894917 0.59398496]\n",
      " [0.86293547 0.6137931 ]\n",
      " [0.91096272 0.69172932]\n",
      " [0.93318104 0.66896552]\n",
      " [0.95784543 0.68224299]\n",
      " [0.96630497 0.73793103]\n",
      " [0.97161937 0.75172414]\n",
      " [0.98330551 0.73451327]\n",
      " [0.98720089 0.77876106]\n",
      " [0.98942682 0.7699115 ]\n",
      " [0.99443517 0.80530973]\n",
      " [0.99648712 0.81308411]\n",
      " [0.99648712 0.82068966]\n",
      " [0.99824356 0.81168831]\n",
      " [0.99733333 0.85981308]\n",
      " [0.99843505 0.8411215 ]\n",
      " [0.99824356 0.84137931]]\n",
      "f1 measure\n",
      "[[0.02788155 0.03126521]\n",
      " [0.0402193  0.0362217 ]\n",
      " [0.08511826 0.06173296]\n",
      " [0.19812087 0.08686081]\n",
      " [0.33370047 0.20412151]\n",
      " [0.5060822  0.30047471]\n",
      " [0.65639345 0.44307966]\n",
      " [0.73144748 0.45301972]\n",
      " [0.83671805 0.56881358]\n",
      " [0.86108373 0.58891698]\n",
      " [0.91020162 0.67017544]\n",
      " [0.92950254 0.65865185]\n",
      " [0.9572594  0.67040498]\n",
      " [0.96622075 0.72686118]\n",
      " [0.97128965 0.73253273]\n",
      " [0.98325966 0.72926339]\n",
      " [0.98712566 0.75992415]\n",
      " [0.98920681 0.75073746]\n",
      " [0.99439467 0.79715668]\n",
      " [0.99644228 0.77994363]\n",
      " [0.99647303 0.79339017]\n",
      " [0.99823978 0.80210491]\n",
      " [0.99730032 0.83587005]\n",
      " [0.99841789 0.82956279]\n",
      " [0.99823223 0.83508426]]\n"
     ]
    }
   ],
   "source": [
    "dirname_results = '../results/'\n",
    "filename_results = 'll_autoencoder+LR.npz'\n",
    "df = np.load(\n",
    "    dirname_results+filename_results\n",
    ")    \n",
    "accuracies = df['acc']\n",
    "f1s = df['f1']\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=1)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier with encoded data\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 / 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hariyuki/apd/lib/python2.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 2 / 25\n",
      "CV 3 / 25\n",
      "CV 4 / 25\n",
      "CV 5 / 25\n",
      "CV 6 / 25\n",
      "CV 7 / 25\n",
      "CV 8 / 25\n",
      "CV 9 / 25\n",
      "CV 10 / 25\n",
      "CV 11 / 25\n",
      "CV 12 / 25\n",
      "CV 13 / 25\n",
      "CV 14 / 25\n",
      "CV 15 / 25\n",
      "CV 16 / 25\n",
      "CV 17 / 25\n",
      "CV 18 / 25\n",
      "CV 19 / 25\n",
      "CV 20 / 25\n",
      "CV 21 / 25\n",
      "CV 22 / 25\n",
      "CV 23 / 25\n",
      "CV 24 / 25\n",
      "CV 25 / 25\n",
      "accuracies\n",
      "[0.99839056 0.83185841 0.72727273]\n",
      "f1 measure\n",
      "[0.99839192 0.819094   0.78400072]\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+LR'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "test_indices = test_indices.tolist()\n",
    "train_indices = train_indices.tolist()\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "# correct label in the end\n",
    "predicted_probas_test = []\n",
    "predicted_probas_test2 = []\n",
    "for k in xrange(len(train_indices)):\n",
    "    print \"CV %d / %d\" % (k+1, len(train_indices))\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    \n",
    "    classifier = LogisticRegression(\n",
    "        penalty='l1', dual=False, tol=0.0001, C=1000.0, fit_intercept=True,\n",
    "        intercept_scaling=1, class_weight=None, random_state=None,\n",
    "        solver='saga', max_iter=1000, multi_class='multinomial', verbose=0,\n",
    "        warm_start=False, n_jobs=1\n",
    "    )\n",
    "    \n",
    "    tic = time.clock();\n",
    "    classifier.fit(X[k][train_index], y[train_index])\n",
    "    toc = time.clock();\n",
    "    \n",
    "    tms_loc = [toc-tic]\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = classifier.predict(X[k][train_index])\n",
    "    toc = time.clock()\n",
    "    tms_loc.append(toc-tic)\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    tic = time.clock()\n",
    "    predict_test = classifier.predict(X[k][test_index])\n",
    "    toc = time.clock()\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "    tms_loc.append(toc-tic)\n",
    "    \n",
    "    tmp = reshape(np.array(y[test_index]), [-1, 1])\n",
    "    tmp = np.hstack([classifier.predict_proba(X[k][test_index]), tmp])\n",
    "    predicted_probas_test.append( tmp.copy() )\n",
    "    tmp = reshape(np.array(y_test2), [-1, 1])\n",
    "    tmp = np.hstack([classifier.predict_proba(X_test2[k]), tmp])\n",
    "    predicted_probas_test2.append( tmp.copy() )\n",
    "    \n",
    "    predict_test2 = classifier.predict(X_test2[k])\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y, confusion_matrices=confusion_matrices,\n",
    "        acc=accuracies, f1=f1s, predicted_probas_test=predicted_probas_test,\n",
    "        predicted_probas_test2=predicted_probas_test2\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "[0.99839056 0.83185841 0.72727273]\n",
      "f1 measure\n",
      "[0.99839192 0.819094   0.78400072]\n"
     ]
    }
   ],
   "source": [
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+LR.npz'\n",
    "\n",
    "df = np.load(\n",
    "    dirname_results+filename_results\n",
    ")\n",
    "accuracies = df['acc']\n",
    "f1s = df['f1']\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes classifier with encoded data\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "[0.90111176 0.6728972  0.72727273]\n",
      "f1 measure\n",
      "[0.90479389 0.66350767 0.80925325]\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+NB'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "# correct label in the end\n",
    "predicted_probas_test = []\n",
    "predicted_probas_test2 = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for k in xrange(len(train_indices)):\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    \n",
    "    classifier = GaussianNB()\n",
    "    \n",
    "    tic = time.clock();\n",
    "    classifier.fit(X[k][train_index], y[train_index])\n",
    "    toc = time.clock();\n",
    "    \n",
    "    tms_loc = [toc-tic]\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = classifier.predict(X[k][train_index])\n",
    "    toc = time.clock()\n",
    "    tms_loc.append(toc-tic)\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    tic = time.clock()\n",
    "    predict_test = classifier.predict(X[k][test_index])\n",
    "    toc = time.clock()\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "    tms_loc.append(toc-tic)\n",
    "    predict_test2 = classifier.predict(X_test2[k])\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    \n",
    "    tmp = reshape(np.array(y[test_index]), [-1, 1])\n",
    "    tmp = np.hstack([classifier.predict_proba(X[k][test_index]), tmp])\n",
    "    predicted_probas_test.append( tmp.copy() )\n",
    "    tmp = reshape(np.array(y_test2), [-1, 1])\n",
    "    tmp = np.hstack([classifier.predict_proba(X_test2[k]), tmp])\n",
    "    predicted_probas_test2.append( tmp.copy() )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y, confusion_matrices=confusion_matrices,\n",
    "        acc=accuracies, f1=f1s, predicted_probas_test=predicted_probas_test,\n",
    "        predicted_probas_test2=predicted_probas_test2\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "[0.90111176 0.6728972  0.72727273]\n",
      "f1 measure\n",
      "[0.90479389 0.66350767 0.80925325]\n"
     ]
    }
   ],
   "source": [
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+NB.npz'\n",
    "\n",
    "df = np.load(\n",
    "    dirname_results+filename_results\n",
    ")\n",
    "accuracies = df['acc']\n",
    "f1s = df['f1']\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Bayesian classifier with bnlearn\n",
    "\n",
    "To reproduce this part of research, user should additionally install kernel for R (please see [Chemfin notebook](../Chemfin.ipynb) ) and bnlearn package.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from computational_utils import reshape\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "rpy2.robjects.pandas2ri.activate()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install bnlearn package, run code in the following cell. Otherwise please skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils = importr('utils')\n",
    "utils.install_packages('bnlearn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn structure, fit training set and predict labels for training, validation and test2 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnlearn = importr('bnlearn')\n",
    "\n",
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+HBN'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "#filename_cv = 'cv_indices.npz'\n",
    "filename_cv = 'physical_cv_indices_nc.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for k in xrange(len(train_indices)):\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    dataset_train = np.hstack([y[train_index], X[k, train_index, :]])\n",
    "    dataset_train = pd.DataFrame(dataset_train, columns=colnames)\n",
    "    dataset_train['identity'] = dataset_train['identity'].apply(str)\n",
    "    dmap = dataset_train.iloc[:, 0].values\n",
    "    dmap = np.unique(dmap)\n",
    "    dmap = np.array(map(float, dmap)).astype('i')\n",
    "    dataset_test = np.hstack([y[test_index], X[k, test_index, :]])\n",
    "    dataset_test = pd.DataFrame(dataset_test, columns=colnames)\n",
    "    dataset_test['identity'] = dataset_test['identity'].apply(str)\n",
    "    dataset_test2 = np.hstack([y_test2, X_test2[k, :, :]])\n",
    "    dataset_test2 = pd.DataFrame(dataset_test2, columns=colnames)\n",
    "    dataset_test2['identity'] = dataset_test2['identity'].apply(str)\n",
    "    \n",
    "    tic = time.clock()\n",
    "    hBN_structure = bnlearn.mmhc(dataset_train)\n",
    "    toc = time.clock()\n",
    "    tms_loc = [toc-tic]\n",
    "    fitted_bn = bnlearn.bn_fit(hBN_structure, dataset_train, method='mle')\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_train.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    \n",
    "    predict_test = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_test.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    \n",
    "    toc = time.clock()\n",
    "    predict_train = np.array(predict_train)-1\n",
    "    predict_train = dmap[predict_train]\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    predict_test = np.array(predict_test)-1\n",
    "    predict_test = dmap[predict_test]\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    tms_loc.append(toc-tic)\n",
    "    predict_test2 = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_test2.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    predict_test2 = np.array(predict_test2)-1\n",
    "    predict_test2 = dmap[predict_test2]\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "[0.92175274 0.68831169 0.63636364]\n",
      "f1 measure\n",
      "[0.92322622 0.6599836  0.72402597]\n"
     ]
    }
   ],
   "source": [
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+HBN.npz'\n",
    "\n",
    "df = np.load(\n",
    "    dirname_results+filename_results\n",
    ")\n",
    "\n",
    "accuracies = []\n",
    "f1s = []\n",
    "y = df['y']\n",
    "y_test2 = df['y_test2'].flatten()\n",
    "for k in xrange(len(train_indices)):\n",
    "    acc_loc = []\n",
    "    f1_loc = []\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    \n",
    "    predict_train = df['predict_train'][k]\n",
    "    predict_test = df['predict_test'][k]\n",
    "    predict_test2 = df['predict_test2'][k]\n",
    "    \n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    \n",
    "\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder learned on whole database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Errors on input set (2263 samples): \n",
      "min=5.964e-02 / mean=1.099e-01 / median=1.061e-01 / max=2.278e-01\n",
      "(2) Errors on input set (2263 samples): \n",
      "min=4.737e-02 / mean=8.862e-02 / median=8.615e-02 / max=1.804e-01\n",
      "(3) Errors on input set (2263 samples): \n",
      "min=5.128e-02 / mean=1.058e-01 / median=1.013e-01 / max=5.210e-01\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_filename_prefix = 'full_data_model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 14\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "\n",
    "learning_rate = 0.0025\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-5\n",
    "optimizer = lambda params: torch.optim.Adam(params, lr=learning_rate, betas=betas, eps=eps) # Sparse\n",
    "N = T.shape[1]\n",
    "# AE structure + instance\n",
    "nls = [nn.ReLU()]+[nn.Sigmoid()]*len(sizes)\n",
    "sizes = [N] + sizes\n",
    "dataset = torch.from_numpy(T.copy())\n",
    "data = ae.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "times = np.zeros([len(sizes)-1, 2])\n",
    "# Nlevels, l1/l2\n",
    "stats_integral = np.zeros([len(sizes)-1])\n",
    "# N samples, Nlevels, l1/l2\n",
    "sample_stats = np.zeros([len(sizes)-1, T.shape[0]])\n",
    "# Ntrain, Nlevels, nEpoch\n",
    "loss_values = np.zeros([len(sizes)-1, max(nEpoch)])\n",
    "for k in xrange(len(sizes)-1):\n",
    "    autoencoder = ae.AutoEncoder(sizes=sizes[:k+2], nls=nls[:k+2], optimizer=optimizer, loss=nn.SmoothL1Loss)\n",
    "    if k > 0:\n",
    "        for i in xrange(k):\n",
    "            autoencoder.Encoder[2*i].weight = weights[2*i]\n",
    "            autoencoder.Encoder[2*i].bias = biases[2*i]\n",
    "            autoencoder.Decoder[2*(i+1)].weight = weights[2*i+1]\n",
    "            autoencoder.Decoder[2*(i+1)].bias = biases[2*i+1]\n",
    "    t1_time = time.time(); t1_clock = time.clock()\n",
    "    loss_values_level = autoencoder.fit(data, nEpoch[k], verbose=0)\n",
    "    t2_clock = time.clock(); t2_time = time.time()\n",
    "    times[k, 0] = t2_time-t1_time\n",
    "    times[k, 1] = t2_clock-t1_clock\n",
    "    torch.save(autoencoder.state_dict(), model_dirname+model_filename_prefix)\n",
    "    print '(%d) Errors on input set (%d samples): ' % (k+1, T.shape[0])\n",
    "    stats_level = ae.getStats(autoencoder, T)\n",
    "\n",
    "    stats_integral[k] = stats_level[0]\n",
    "    sample_stats[k] = stats_level[1]\n",
    "    loss_values[k, :nEpoch[k]] = loss_values_level\n",
    "    np.savez_compressed(\n",
    "        model_dirname+'full_ae_stats', stats_integral=stats_integral,\n",
    "        loss_values=loss_values,\n",
    "        sample_stats=sample_stats, nEpoch=nEpoch, times=times\n",
    "    )\n",
    "    # test on train/valid. sets\n",
    "    if k < (len(sizes)-2):\n",
    "        weights, biases = [], []\n",
    "        for i in xrange(k+1):\n",
    "            weights.append( copy.deepcopy(autoencoder.Encoder[2*i].weight) )\n",
    "            weights.append( copy.deepcopy(autoencoder.Decoder[2*i].weight) )\n",
    "            biases.append( copy.deepcopy(autoencoder.Encoder[2*i].bias) )\n",
    "            biases.append( copy.deepcopy(autoencoder.Decoder[2*i].bias) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded with full_data_model_ae_. Comp.time=0.47915 s\n"
     ]
    }
   ],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_fname = 'full_data_model_ae_'\n",
    "save_filename = 'full_dataset_autoencoded'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 14\n",
    "\n",
    "resultsTime = []\n",
    "# optimizer parameters\n",
    "learning_rate = 0.0025\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-5\n",
    "optimizer = lambda params: torch.optim.Adam(params, lr=learning_rate, betas=betas, eps=eps)\n",
    "    \n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, y = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "    \n",
    "    \n",
    "N = T.shape[1]\n",
    "sizes = [N] + sizes\n",
    "\n",
    "nls = [nn.ReLU()]+[nn.Sigmoid()]*(len(sizes)-1)\n",
    "\n",
    "autoencoder = ae.AutoEncoder(sizes, nls, optimizer=optimizer, loss=nn.SmoothL1Loss)\n",
    "autoencoder.load_state_dict(torch.load(model_dirname+model_fname))\n",
    "\n",
    "X = Variable(torch.from_numpy(T.copy()))\n",
    "tic = time.clock()\n",
    "Y = autoencoder.encode(X)\n",
    "toc = time.clock()\n",
    "X = Y.data.numpy()\n",
    "resultsTime = toc-tic\n",
    "print \"encoded with %s. Comp.time=%.5f s\" % (model_fname, resultsTime)\n",
    "np.savez_compressed(data_dirname+save_filename, data=X, label=y, time=resultsTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
