{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [1. Imports](#Imports)\n",
    "- [2. Build models](#Build-models)\n",
    "- [3. Approximation error](#Approximation-error)\n",
    "- [4. Encode data with computed autoencoders](#Encode-data-with-computed-autoencoders)\n",
    "- [5. Investigate how size of last encoding layer affects results: training models](#Investigate-how-size-of-last-encoding-layer-affects-results:-training-models)\n",
    "- [6. Investigate how size of last encoding layer affects results: encode data](#Investigate-how-size-of-last-encoding-layer-affects-results:-encode-data)\n",
    "- [7. Investigate how size of last encoding layer affects results: predict with logistic regression](#Investigate-how-size-of-last-encoding-layer-affects-results:-predict-with-logistic-regression)\n",
    "- [8. Logistic regression classifier with encoded data](#Logistic-regression-classifier-with-encoded-data)\n",
    "- [9. Gaussian Naive Bayes classifier with encoded data](#Gaussian-Naive-Bayes-classifier-with-encoded-data)\n",
    "- [10. Hybrid Bayesian classifier with bnlearn](#Hybrid-Bayesian-classifier-with-bnlearn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Back to Chemfin](../Chemfin.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The first cell with code includes all necessary inputs.\n",
    "\n",
    "Requires [numpy](http://www.numpy.org/), [scikit-learn](http://scikit-learn.org/), [pyTorch](http://pytorch.org/), [Rpy2](https://rpy2.readthedocs.io).\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import autoencoder as ae\n",
    "\n",
    "random_state = 150\n",
    "torch.manual_seed(random_state);\n",
    "\n",
    "\n",
    "from computational_utils import reshape\n",
    "\n",
    "from io_work import stringSplitByNumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models\n",
    "\n",
    "Next cell contains script to build autoencoder models relevant to CV indexes supplied by data/cv_indices.npz.\n",
    "\n",
    "Parameters to control are:\n",
    "\n",
    "- sizes: list of integers which specifies output sizes for each encoding layer\n",
    "- batch_size: number of samples to be used for computing new update at each epoch\n",
    "- nEpoch: number of epochs for each layer\n",
    "- num_workers: number of parallel processes to work\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "filename_cv = 'cv_indices.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 2\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "\n",
    "ae.buildAutoencoderModels(\n",
    "    T, train_indices, test_indices, sizes, model_dirname, nEpoch,\n",
    "    batch_size, num_workers, model_filename_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation error\n",
    "\n",
    "In this code data encoded and decoded with previously trained models. Resulting approximation (relative residual error by means of $l_2$ norm) is printed sample-wise.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "filename_cv = 'cv_indices.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "N = T.shape[1]\n",
    "\n",
    "ae.checkRelRes(T, train_indices, test_indices, sizes, model_dirname, model_filename_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data with computed autoencoders\n",
    "\n",
    "It will produce data encoded with models from previous steps.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset = 'dataset.npz'\n",
    "save_filename = 'autoencoded_' + filename_dataset\n",
    "filename_dataset2 = 'test2.npz'\n",
    "save_filename2 = 'autoencoded_' + filename_dataset2\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "num_workers = 2\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "ae.encodeDataset(df, sizes, model_dirname, model_filename_prefix,\n",
    "                  data_dirname+save_filename, num_workers, return_result=0)\n",
    "df = np.load(data_dirname+filename_dataset2)\n",
    "ae.encodeDataset(df, sizes, model_dirname, model_filename_prefix,\n",
    "                  data_dirname+save_filename2, num_workers, return_result=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: training models\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "\n",
    "filename_dataset = 'dataset.npz'\n",
    "filename_cv = 'cv_indices.npz'\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "sizes_ll = range(1, sizes[-1]+1)\n",
    "nEpoch = [1000, 1000, 1000]\n",
    "batch_size = 200\n",
    "num_workers = 2\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "T, labels = df['data'], df['label']\n",
    "# unfold into matrix\n",
    "T = reshape(T, [T.shape[0], -1])\n",
    "# normalize among samples\n",
    "T /= np.linalg.norm(T, axis=1, keepdims=1)\n",
    "\n",
    "ae.investigateLastLayerTrain(T, train_indices[:n_splits], test_indices[:n_splits], sizes, sizes_ll,\n",
    "        model_dirname, nEpoch, batch_size, num_workers, model_filename_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: encode data\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_prefix = 'll='\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset = 'dataset.npz'\n",
    "save_filename_postfix = 'autoencoded_' + filename_dataset\n",
    "\n",
    "sizes = [400, 100, 25]\n",
    "num_workers = 2\n",
    "\n",
    "df = np.load(data_dirname+filename_dataset)\n",
    "\n",
    "for k in xrange(sizes[-1]):\n",
    "    sizes_ll = sizes[:-1] + [k+1]\n",
    "    model_full_prefix = model_prefix + str(k+1) + '_' + model_filename_prefix\n",
    "    save_filename = model_prefix+str(k+1)+'_'+save_filename_postfix\n",
    "    ae.encodeDataset(\n",
    "        df, sizes_ll, model_dirname, model_full_prefix,\n",
    "        data_dirname+save_filename, num_workers, return_result=0\n",
    "    )\n",
    "    print model_dirname+model_full_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate how size of last encoding layer affects results: predict with logistic regression\n",
    "\n",
    "Here we use only one repeat of 5-fold CV\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from computational_utils import reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "model_prefix = 'll='\n",
    "model_dirname = '../models/autoencoder/last_layer/'\n",
    "model_filename_prefix = 'model_ae_'\n",
    "filename_dataset_base = 'autoencoded_dataset.npz'\n",
    "\n",
    "ll_max = 25\n",
    "n_splits = 5\n",
    "\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'll_autoencoder+LR'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "\n",
    "\n",
    "tms = []\n",
    "#predict_train_all = []\n",
    "#predict_test_all = []\n",
    "\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for l in xrange(ll_max):\n",
    "    print 'last layer size=%d' % (l+1)\n",
    "    filename_data = model_prefix+str(l+1)+'_'+filename_dataset_base\n",
    "    df = np.load(data_dirname+filename_data)\n",
    "    X, y = df['data'], df['label']\n",
    "    y = reshape(y, [-1, 1])\n",
    "    \n",
    "    tms_l = []\n",
    "    predict_train_l = []\n",
    "    predict_test_l = []\n",
    "\n",
    "    accuracies_l = []\n",
    "    f1s_l = []\n",
    "    for k in xrange(n_splits):\n",
    "        train_index = train_indices[k]\n",
    "        test_index = test_indices[k]\n",
    "\n",
    "        classifier = LogisticRegression(\n",
    "            penalty='l1', dual=False, tol=0.0001, C=1000.0, fit_intercept=True,\n",
    "            intercept_scaling=1, class_weight=None, random_state=None,\n",
    "            solver='saga', max_iter=1000, multi_class='multinomial', verbose=0,\n",
    "            warm_start=False, n_jobs=1\n",
    "        )\n",
    "\n",
    "        tic = time.clock();\n",
    "        classifier.fit(X[k][train_index], y[train_index])\n",
    "        toc = time.clock();\n",
    "\n",
    "        tms_loc = [toc-tic]\n",
    "\n",
    "        tic = time.clock()\n",
    "        predict_train = classifier.predict(X[k][train_index])\n",
    "        toc = time.clock()\n",
    "        tms_loc.append(toc-tic)\n",
    "        acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "        f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "        tic = time.clock()\n",
    "        predict_test = classifier.predict(X[k][test_index])\n",
    "        toc = time.clock()\n",
    "        acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "        f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "        #confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "        tms_loc.append(toc-tic)\n",
    "\n",
    "        accuracies_l.append(acc_loc)\n",
    "        f1s_l.append(f1_loc)\n",
    "        tms_l.append(tms_loc)\n",
    "        #predict_train_l.append( predict_train )\n",
    "        #predict_test_l.append( predict_test )\n",
    "        \n",
    "    tms.append(tms_l)\n",
    "    #predict_train_all.append(predict_train_l)\n",
    "    #predict_test_all.append(predict_test_l)\n",
    "\n",
    "    accuracies.append(accuracies_l)\n",
    "    f1s.append(f1s_l)\n",
    "np.savez_compressed(\n",
    "    dirname_results+filename_results, tms=tms,\n",
    "    acc=accuracies, f1=f1s\n",
    ")    \n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=1)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier with encoded data\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+LR'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for k in xrange(len(train_indices)):\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    \n",
    "    classifier = LogisticRegression(\n",
    "        penalty='l1', dual=False, tol=0.0001, C=1000.0, fit_intercept=True,\n",
    "        intercept_scaling=1, class_weight=None, random_state=None,\n",
    "        solver='saga', max_iter=1000, multi_class='multinomial', verbose=0,\n",
    "        warm_start=False, n_jobs=1\n",
    "    )\n",
    "    \n",
    "    tic = time.clock();\n",
    "    classifier.fit(X[k][train_index], y[train_index])\n",
    "    toc = time.clock();\n",
    "    \n",
    "    tms_loc = [toc-tic]\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = classifier.predict(X[k][train_index])\n",
    "    toc = time.clock()\n",
    "    tms_loc.append(toc-tic)\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    tic = time.clock()\n",
    "    predict_test = classifier.predict(X[k][test_index])\n",
    "    toc = time.clock()\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "    tms_loc.append(toc-tic)\n",
    "    predict_test2 = classifier.predict(X_test2[k])\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y, confusion_matrices=confusion_matrices,\n",
    "        acc=accuracies, f1=f1s\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes classifier with encoded data\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+NB'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for k in xrange(len(train_indices)):\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    \n",
    "    classifier = GaussianNB()\n",
    "    \n",
    "    tic = time.clock();\n",
    "    classifier.fit(X[k][train_index], y[train_index])\n",
    "    toc = time.clock();\n",
    "    \n",
    "    tms_loc = [toc-tic]\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = classifier.predict(X[k][train_index])\n",
    "    toc = time.clock()\n",
    "    tms_loc.append(toc-tic)\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    tic = time.clock()\n",
    "    predict_test = classifier.predict(X[k][test_index])\n",
    "    toc = time.clock()\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    confusion_matrices.append(confusion_matrix(y[test_index], predict_test))\n",
    "    tms_loc.append(toc-tic)\n",
    "    predict_test2 = classifier.predict(X_test2[k])\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y, confusion_matrices=confusion_matrices,\n",
    "        acc=accuracies, f1=f1s\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Bayesian classifier with bnlearn\n",
    "\n",
    "To reproduce this part of research, user should additionally install kernel for R (please see [Chemfin notebook](../Chemfin.ipynb) ) and bnlearn package.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from computational_utils import reshape\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "rpy2.robjects.pandas2ri.activate()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install bnlearn package, run code in the following cell. Otherwise please skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = importr('utils')\n",
    "utils.install_packages('bnlearn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn structure, fit training set and predict labels for training, validation and test2 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnlearn = importr('bnlearn')\n",
    "\n",
    "data_dirname = '../data/'\n",
    "dirname_results = '../results/'\n",
    "filename_results = 'autoencoder+HBN'\n",
    "data_filename = 'autoencoded_dataset.npz'\n",
    "data_test2_filename = 'autoencoded_test2.npz'\n",
    "\n",
    "filename_cv = 'cv_indices.npz'\n",
    "df = np.load(data_dirname+filename_cv)\n",
    "test_indices, train_indices = df['test_indices'], df['train_indices']\n",
    "\n",
    "df = np.load(data_dirname+data_test2_filename)\n",
    "X_test2, y_test2 = df['data'], df['label']\n",
    "y_test2 = reshape(y_test2, [-1, 1])\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "X, y = df['data'], df['label']\n",
    "y = reshape(y, [-1, 1])\n",
    "colnames = ['identity'] + ['V%d' % (i) for i in xrange(X.shape[-1])]\n",
    "\n",
    "tms = []\n",
    "predict_train_all = []\n",
    "predict_test_all = []\n",
    "predict_test2_all = []\n",
    "\n",
    "confusion_matrices = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for k in xrange(len(train_indices)):\n",
    "    train_index = train_indices[k]\n",
    "    test_index = test_indices[k]\n",
    "    dataset_train = np.hstack([y[train_index], X[k, train_index, :]])\n",
    "    dataset_train = pd.DataFrame(dataset_train, columns=colnames)\n",
    "    dataset_train['identity'] = dataset_train['identity'].apply(str)\n",
    "    dmap = dataset_train.iloc[:, 0].values\n",
    "    dmap = np.unique(dmap)\n",
    "    dataset_test = np.hstack([y[test_index], X[k, test_index, :]])\n",
    "    dataset_test = pd.DataFrame(dataset_test, columns=colnames)\n",
    "    dataset_test['identity'] = dataset_test['identity'].apply(str)\n",
    "    dataset_test2 = np.hstack([y_test2, X_test2[k, :, :]])\n",
    "    dataset_test2 = pd.DataFrame(dataset_test2, columns=colnames)\n",
    "    dataset_test2['identity'] = dataset_test2['identity'].apply(str)\n",
    "    \n",
    "    tic = time.clock()\n",
    "    hBN_structure = bnlearn.mmhc(dataset_train)\n",
    "    toc = time.clock()\n",
    "    tms_loc = [toc-tic]\n",
    "    fitted_bn = bnlearn.bn_fit(hBN_structure, dataset_train, method='mle')\n",
    "    \n",
    "    tic = time.clock()\n",
    "    predict_train = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_train.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    \n",
    "    predict_test = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_test.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    \n",
    "    toc = time.clock()\n",
    "    predict_train = np.array(predict_train)\n",
    "    predict_train = dmap[predict_train]\n",
    "    acc_loc = [accuracy_score(y[train_index], predict_train)]\n",
    "    f1_loc = [f1_score(y[train_index], predict_train, average='weighted')]\n",
    "    predict_test = np.array(predict_test)\n",
    "    predict_test = dmap[predict_test]\n",
    "    acc_loc.append( accuracy_score(y[test_index], predict_test) )\n",
    "    f1_loc.append(f1_score(y[test_index], predict_test, average='weighted') )\n",
    "    tms_loc.append(toc-tic)\n",
    "    predict_test2 = bnlearn.predict_bn_fit(\n",
    "        fitted_bn, node='identity', data=dataset_test2.iloc[:, 1:], method='bayes-lw'\n",
    "    )\n",
    "    predict_test2 = np.array(predict_test2)\n",
    "    predict_test2 = dmap[predict_test2]\n",
    "    acc_loc.append( accuracy_score(y_test2, predict_test2) )\n",
    "    f1_loc.append(f1_score(y_test2, predict_test2, average='weighted') )\n",
    "    tms.append(tms_loc)\n",
    "    predict_train_all.append( predict_train )\n",
    "    predict_test_all.append( predict_test )\n",
    "    predict_test2_all.append( predict_test2 )\n",
    "    \n",
    "    accuracies.append(acc_loc)\n",
    "    f1s.append(f1_loc)\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        dirname_results+filename_results, tms=tms, predict_train=predict_train_all,\n",
    "        predict_test=predict_test_all, predict_test2=predict_test2_all, test_indices=test_indices,\n",
    "        train_indices=train_indices, y_test2=y_test2.T, y=y\n",
    "    )\n",
    "accuracies = np.array(accuracies)\n",
    "f1s = np.array(f1s)\n",
    "print \"accuracies\"\n",
    "print np.median(accuracies, axis=0)\n",
    "print \"f1 measure\"\n",
    "print np.median(f1s, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
